{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.b.i.A Data inputation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The work was done,and we use the UTX stock for example. The result data was saved in csv. The above result were the same, I have used the UTX stock for example!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>classes</th>\n",
       "      <th>conversation</th>\n",
       "      <th>created_at</th>\n",
       "      <th>direct</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to_message_id</th>\n",
       "      <th>investor_relations</th>\n",
       "      <th>link_embed</th>\n",
       "      <th>links</th>\n",
       "      <th>...</th>\n",
       "      <th>source</th>\n",
       "      <th>structurable</th>\n",
       "      <th>total_likes</th>\n",
       "      <th>total_reshares</th>\n",
       "      <th>user</th>\n",
       "      <th>view_chart</th>\n",
       "      <th>day</th>\n",
       "      <th>judge_sentiment</th>\n",
       "      <th>s_t</th>\n",
       "      <th>3_moving_window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4398</th>\n",
       "      <td>Bullish idea for Thur $UTX on decent RS, good ...</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thu, 05 Dec 2013 13:57:02 -0000</td>\n",
       "      <td>False</td>\n",
       "      <td>17913774</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'username': 'OutsizedReturns', 'id': 251951, ...</td>\n",
       "      <td>{'path': '/message/17913774#17913774', 'image'...</td>\n",
       "      <td>20131205</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4362</th>\n",
       "      <td>$UTX on the move again</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon, 16 Dec 2013 18:26:53 -0000</td>\n",
       "      <td>False</td>\n",
       "      <td>18218533</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'username': 'Reno511', 'id': 269611, 'name': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20131216</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4351</th>\n",
       "      <td>$UTX engine delivery news from a good payer, g...</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun, 22 Dec 2013 23:42:17 -0000</td>\n",
       "      <td>False</td>\n",
       "      <td>18394295</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'username': 'DSD_2016', 'id': 82096, 'name': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20131222</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>$UTX My portfolio loves you and your dividend ...</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon, 23 Dec 2013 21:37:44 -0000</td>\n",
       "      <td>False</td>\n",
       "      <td>18424770</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'username': 'signupmoney', 'id': 16466, 'name...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20131223</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4345</th>\n",
       "      <td>$UTX $GE $UNP all looking ripe for breakouts. ...</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue, 24 Dec 2013 13:22:58 -0000</td>\n",
       "      <td>False</td>\n",
       "      <td>18432833</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>{'url': 'http://www.stocktwits.com/mobile', 'i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'username': 'simonsays452', 'id': 4478, 'name...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20131224</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body  classes conversation  \\\n",
       "4398  Bullish idea for Thur $UTX on decent RS, good ...  default          NaN   \n",
       "4362                             $UTX on the move again  default          NaN   \n",
       "4351  $UTX engine delivery news from a good payer, g...  default          NaN   \n",
       "4347  $UTX My portfolio loves you and your dividend ...  default          NaN   \n",
       "4345  $UTX $GE $UNP all looking ripe for breakouts. ...  default          NaN   \n",
       "\n",
       "                           created_at  direct        id  \\\n",
       "4398  Thu, 05 Dec 2013 13:57:02 -0000   False  17913774   \n",
       "4362  Mon, 16 Dec 2013 18:26:53 -0000   False  18218533   \n",
       "4351  Sun, 22 Dec 2013 23:42:17 -0000   False  18394295   \n",
       "4347  Mon, 23 Dec 2013 21:37:44 -0000   False  18424770   \n",
       "4345  Tue, 24 Dec 2013 13:22:58 -0000   False  18432833   \n",
       "\n",
       "     in_reply_to_message_id  investor_relations link_embed links  \\\n",
       "4398                   None               False        NaN    []   \n",
       "4362                   None               False        NaN    []   \n",
       "4351                   None               False        NaN    []   \n",
       "4347                   None               False        NaN    []   \n",
       "4345                   None               False        NaN    []   \n",
       "\n",
       "           ...                                                    source  \\\n",
       "4398       ...                                                      None   \n",
       "4362       ...                                                      None   \n",
       "4351       ...                                                      None   \n",
       "4347       ...                                                      None   \n",
       "4345       ...         {'url': 'http://www.stocktwits.com/mobile', 'i...   \n",
       "\n",
       "     structurable total_likes total_reshares  \\\n",
       "4398          NaN           0              0   \n",
       "4362          NaN           0              0   \n",
       "4351          NaN           0              0   \n",
       "4347          NaN           0              0   \n",
       "4345          NaN           0              0   \n",
       "\n",
       "                                                   user  \\\n",
       "4398  {'username': 'OutsizedReturns', 'id': 251951, ...   \n",
       "4362  {'username': 'Reno511', 'id': 269611, 'name': ...   \n",
       "4351  {'username': 'DSD_2016', 'id': 82096, 'name': ...   \n",
       "4347  {'username': 'signupmoney', 'id': 16466, 'name...   \n",
       "4345  {'username': 'simonsays452', 'id': 4478, 'name...   \n",
       "\n",
       "                                             view_chart       day  \\\n",
       "4398  {'path': '/message/17913774#17913774', 'image'...  20131205   \n",
       "4362                                                NaN  20131216   \n",
       "4351                                                NaN  20131222   \n",
       "4347                                                NaN  20131223   \n",
       "4345                                                NaN  20131224   \n",
       "\n",
       "     judge_sentiment  s_t  3_moving_window  \n",
       "4398               1  0.0              NaN  \n",
       "4362               1  0.0              NaN  \n",
       "4351               1  0.0              0.0  \n",
       "4347               1  0.0              0.0  \n",
       "4345               1  0.0              0.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xlwt,json\n",
    "\n",
    "with open('./twits/UTX.txt','r')as file_open:\n",
    "    data=json.load(file_open)\n",
    "df = pd.DataFrame(data)\n",
    "#date extration\n",
    "dict_month = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
    "              'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}\n",
    "df['day'] = df['created_at'].apply(lambda x: int(x.split(',')[1].split(' ')[1]) + dict_month[x.split(',')[1].split(' ')[2]] * 100 + int(x.split(',')[1].split(' ')[3]) * 10000)\n",
    "df = df.sort_values(by = \"day\",ascending= True) \n",
    "#sentiment judgement\n",
    "def judge_sentiment1(x):\n",
    "    if x is None:\n",
    "        return 0\n",
    "    elif x['class']=='bullish':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "df['judge_sentiment'] = df['sentiment'].apply(lambda x: judge_sentiment1(x))\n",
    "df_taged = df[df['judge_sentiment'] != 0]\n",
    "#s_t calculating\n",
    "df_taged_sentiment = df_taged\n",
    "def calculate_st(x):\n",
    "    taged_days = len(x)\n",
    "    bullis_days = 0\n",
    "    if x['class'] == 'bullish':\n",
    "        bullis_days += 1\n",
    "    return (bullis_days - (taged_days - bullis_days)) / taged_days\n",
    "df_taged_sentiment['s_t'] = df_taged_sentiment['sentiment'].apply(lambda x: calculate_st(x))\n",
    "#3 point moving average\n",
    "df_taged_sentiment['3_moving_window'] = df_taged_sentiment['s_t'].rolling(window=3).mean()\n",
    "df_taged_sentiment.to_csv('./UTX_3mw.csv', index = False)\n",
    "print('The work was done,and we use the UTX stock for example. The result data was saved in csv. The above result were the same, I have used the UTX stock for example!')\n",
    "df_taged_sentiment.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.b.i.B m_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The UTX stock m_v is:\n",
      "          day\n",
      "20150721   88\n",
      "20160222   77\n",
      "20151020   69\n",
      "20160226   68\n",
      "20161025   53\n",
      "20160223   50\n",
      "20140722   43\n",
      "20160427   43\n",
      "20150126   41\n",
      "20160225   40\n",
      "20160301   39\n",
      "20150421   39\n",
      "20160127   38\n",
      "20150722   37\n",
      "20150127   36\n",
      "20160726   33\n",
      "20150720   31\n",
      "20161130   30\n",
      "20141125   30\n",
      "20161201   30\n",
      "20140122   29\n",
      "20151211   27\n",
      "20150615   26\n",
      "20141124   23\n",
      "20131213   23\n",
      "20161010   21\n",
      "20141021   20\n",
      "20160414   19\n",
      "20150313   19\n",
      "20150831   19\n",
      "...       ...\n",
      "20140105    1\n",
      "20161120    1\n",
      "20131203    1\n",
      "20151107    1\n",
      "20141228    1\n",
      "20151119    1\n",
      "20160118    1\n",
      "20140828    1\n",
      "20151105    1\n",
      "20141114    1\n",
      "20150506    1\n",
      "20151125    1\n",
      "20141026    1\n",
      "20160102    1\n",
      "20140908    1\n",
      "20140523    1\n",
      "20140920    1\n",
      "20151227    1\n",
      "20151209    1\n",
      "20151219    1\n",
      "20151225    1\n",
      "20140930    1\n",
      "20141028    1\n",
      "20140303    1\n",
      "20141112    1\n",
      "20141120    1\n",
      "20141216    1\n",
      "20141220    1\n",
      "20151123    1\n",
      "20150215    1\n",
      "\n",
      "[898 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "m_v = pd.DataFrame(df['day'].value_counts())\n",
    "#m_v.to_csv('./UTX_mv.csv')\n",
    "print('The UTX stock m_v is:')\n",
    "print(m_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.b.i.C mv_1_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The UTX stock mv_1_t is:\n",
      "               day\n",
      "20150721       NaN\n",
      "20160222 -0.125000\n",
      "20151020 -0.103896\n",
      "20160226 -0.014493\n",
      "20161025 -0.220588\n",
      "20160223 -0.056604\n",
      "20140722 -0.140000\n",
      "20160427  0.000000\n",
      "20150126 -0.046512\n",
      "20160225 -0.024390\n",
      "20160301 -0.025000\n",
      "20150421  0.000000\n",
      "20160127 -0.025641\n",
      "20150722 -0.026316\n",
      "20150127 -0.027027\n",
      "20160726 -0.083333\n",
      "20150720 -0.060606\n",
      "20161130 -0.032258\n",
      "20141125  0.000000\n",
      "20161201  0.000000\n",
      "20140122 -0.033333\n",
      "20151211 -0.068966\n",
      "20150615 -0.037037\n",
      "20141124 -0.115385\n",
      "20131213  0.000000\n",
      "20161010 -0.086957\n",
      "20141021 -0.047619\n",
      "20160414 -0.050000\n",
      "20150313  0.000000\n",
      "20150831  0.000000\n",
      "...            ...\n",
      "20140105  0.000000\n",
      "20161120  0.000000\n",
      "20131203  0.000000\n",
      "20151107  0.000000\n",
      "20141228  0.000000\n",
      "20151119  0.000000\n",
      "20160118  0.000000\n",
      "20140828  0.000000\n",
      "20151105  0.000000\n",
      "20141114  0.000000\n",
      "20150506  0.000000\n",
      "20151125  0.000000\n",
      "20141026  0.000000\n",
      "20160102  0.000000\n",
      "20140908  0.000000\n",
      "20140523  0.000000\n",
      "20140920  0.000000\n",
      "20151227  0.000000\n",
      "20151209  0.000000\n",
      "20151219  0.000000\n",
      "20151225  0.000000\n",
      "20140930  0.000000\n",
      "20141028  0.000000\n",
      "20140303  0.000000\n",
      "20141112  0.000000\n",
      "20141120  0.000000\n",
      "20141216  0.000000\n",
      "20141220  0.000000\n",
      "20151123  0.000000\n",
      "20150215  0.000000\n",
      "\n",
      "[898 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "mv_1_t = (m_v - m_v.shift(1)) / m_v.shift(1)\n",
    "print('The UTX stock mv_1_t is:')\n",
    "print(mv_1_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.b.i.D mv_10_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The UTX stock mv_10_t is:\n",
      "               day\n",
      "20150721       NaN\n",
      "20160222       NaN\n",
      "20151020       NaN\n",
      "20160226       NaN\n",
      "20161025       NaN\n",
      "20160223       NaN\n",
      "20140722       NaN\n",
      "20160427       NaN\n",
      "20150126       NaN\n",
      "20160225  0.699301\n",
      "20160301  0.745698\n",
      "20150421  0.804124\n",
      "20160127  0.837004\n",
      "20150722  0.874704\n",
      "20150127  0.886700\n",
      "20160726  0.848329\n",
      "20150720  0.822281\n",
      "20161130  0.824176\n",
      "20141125  0.849858\n",
      "20161201  0.874636\n",
      "20140122  0.870871\n",
      "20151211  0.841121\n",
      "20150615  0.841424\n",
      "20141124  0.779661\n",
      "20131213  0.815603\n",
      "20161010  0.777778\n",
      "20141021  0.772201\n",
      "20160414  0.766129\n",
      "20150313  0.801688\n",
      "20150831  0.840708\n",
      "...            ...\n",
      "20140105  1.000000\n",
      "20161120  1.000000\n",
      "20131203  1.000000\n",
      "20151107  1.000000\n",
      "20141228  1.000000\n",
      "20151119  1.000000\n",
      "20160118  1.000000\n",
      "20140828  1.000000\n",
      "20151105  1.000000\n",
      "20141114  1.000000\n",
      "20150506  1.000000\n",
      "20151125  1.000000\n",
      "20141026  1.000000\n",
      "20160102  1.000000\n",
      "20140908  1.000000\n",
      "20140523  1.000000\n",
      "20140920  1.000000\n",
      "20151227  1.000000\n",
      "20151209  1.000000\n",
      "20151219  1.000000\n",
      "20151225  1.000000\n",
      "20140930  1.000000\n",
      "20141028  1.000000\n",
      "20140303  1.000000\n",
      "20141112  1.000000\n",
      "20141120  1.000000\n",
      "20141216  1.000000\n",
      "20141220  1.000000\n",
      "20151123  1.000000\n",
      "20150215  1.000000\n",
      "\n",
      "[898 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "mv_10_t = pd.DataFrame(m_v['day'] / m_v['day'].rolling(window=10).mean())\n",
    "print('The UTX stock mv_10_t is:')\n",
    "print(mv_10_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.b.iii Calculate rt(3) and rt(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>OPEN</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>LOW</th>\n",
       "      <th>VOLUME</th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>3_close</th>\n",
       "      <th>5_close</th>\n",
       "      <th>rt_3</th>\n",
       "      <th>rt_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-08-26</td>\n",
       "      <td>95.285131</td>\n",
       "      <td>95.822211</td>\n",
       "      <td>94.729534</td>\n",
       "      <td>1657200</td>\n",
       "      <td>94.729534</td>\n",
       "      <td>93.025696</td>\n",
       "      <td>95.062894</td>\n",
       "      <td>-0.017986</td>\n",
       "      <td>0.003519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-08-27</td>\n",
       "      <td>93.720194</td>\n",
       "      <td>94.285053</td>\n",
       "      <td>92.321936</td>\n",
       "      <td>3082700</td>\n",
       "      <td>92.544180</td>\n",
       "      <td>92.692336</td>\n",
       "      <td>95.711089</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.034221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-08-28</td>\n",
       "      <td>92.405280</td>\n",
       "      <td>92.951614</td>\n",
       "      <td>92.331198</td>\n",
       "      <td>2397200</td>\n",
       "      <td>92.581221</td>\n",
       "      <td>95.062894</td>\n",
       "      <td>96.007408</td>\n",
       "      <td>0.026805</td>\n",
       "      <td>0.037007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-08-29</td>\n",
       "      <td>92.349722</td>\n",
       "      <td>93.423875</td>\n",
       "      <td>92.136739</td>\n",
       "      <td>2323500</td>\n",
       "      <td>93.025696</td>\n",
       "      <td>95.711089</td>\n",
       "      <td>95.590712</td>\n",
       "      <td>0.028867</td>\n",
       "      <td>0.027573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-08-30</td>\n",
       "      <td>93.275719</td>\n",
       "      <td>93.331277</td>\n",
       "      <td>92.349722</td>\n",
       "      <td>2492500</td>\n",
       "      <td>92.692336</td>\n",
       "      <td>96.007408</td>\n",
       "      <td>96.748209</td>\n",
       "      <td>0.035764</td>\n",
       "      <td>0.043756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2013-09-03</td>\n",
       "      <td>93.618333</td>\n",
       "      <td>95.498106</td>\n",
       "      <td>93.609071</td>\n",
       "      <td>4633500</td>\n",
       "      <td>95.062894</td>\n",
       "      <td>95.590712</td>\n",
       "      <td>98.396483</td>\n",
       "      <td>0.005552</td>\n",
       "      <td>0.035067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       OPEN       HIGH        LOW   VOLUME      CLOSE    3_close  \\\n",
       "0  2013-08-26  95.285131  95.822211  94.729534  1657200  94.729534  93.025696   \n",
       "1  2013-08-27  93.720194  94.285053  92.321936  3082700  92.544180  92.692336   \n",
       "2  2013-08-28  92.405280  92.951614  92.331198  2397200  92.581221  95.062894   \n",
       "3  2013-08-29  92.349722  93.423875  92.136739  2323500  93.025696  95.711089   \n",
       "4  2013-08-30  93.275719  93.331277  92.349722  2492500  92.692336  96.007408   \n",
       "5  2013-09-03  93.618333  95.498106  93.609071  4633500  95.062894  95.590712   \n",
       "\n",
       "     5_close      rt_3      rt_5  \n",
       "0  95.062894 -0.017986  0.003519  \n",
       "1  95.711089  0.001601  0.034221  \n",
       "2  96.007408  0.026805  0.037007  \n",
       "3  95.590712  0.028867  0.027573  \n",
       "4  96.748209  0.035764  0.043756  \n",
       "5  98.396483  0.005552  0.035067  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_data = pd.read_csv('./csv/UTX.csv')\n",
    "p_data['3_close'] = p_data['CLOSE'].shift(-3)\n",
    "p_data['5_close'] = p_data['CLOSE'].shift(-5)\n",
    "p_data['rt_3'] = (p_data['3_close'] - p_data['CLOSE']) / p_data['CLOSE']\n",
    "p_data['rt_5'] = (p_data['5_close'] - p_data['CLOSE']) / p_data['CLOSE']\n",
    "p_data.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.c.ii  stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Fluid. Too fluid. It's difficult make informed...\n",
      "1       Donald Trump's Carrier Air Conditioner deal, l...\n",
      "2       Donald Trump, \"Move manufacturing offshore, fa...\n",
      "3                                                    None\n",
      "4       Earnings estimates UTX thousands professional ...\n",
      "5       Management one important areas company. We loo...\n",
      "6       The devil always details. And that's true much...\n",
      "7       News Corp network leading companies worlds div...\n",
      "8       5 Trade ideas excerpted detailed analysis plan...\n",
      "9       5 Trade ideas excerpted detailed analysis plan...\n",
      "10                                                   None\n",
      "11                                                   None\n",
      "12      Capital Goods stocks experienced great Novembe...\n",
      "13      Ford Motor Co. forge ahead shifting small-car ...\n",
      "14                                                   None\n",
      "15      It's Fix The Tax Code Friday! Earlier week, ai...\n",
      "16                                                   None\n",
      "17      News Corp network leading companies worlds div...\n",
      "18                                                   None\n",
      "19                                                   None\n",
      "20      John Bassett, CEO, Vaughan-Bassett Furniture C...\n",
      "21      Earnings estimates UTX thousands professional ...\n",
      "22                                                   None\n",
      "23      Cautiously Optimistic United Technologies Unit...\n",
      "24      Cautiously Optimistic United Technologies Unit...\n",
      "25                                                   None\n",
      "26                                                   None\n",
      "27      News Corp network leading companies worlds div...\n",
      "28      From Yahoo Singapore Finance: Dec 2 (Reuters) ...\n",
      "29      Timothy A. Clary | AFP | Getty Images It's bad...\n",
      "                              ...                        \n",
      "4380                                                 None\n",
      "4381                                                 None\n",
      "4382                                                 None\n",
      "4383                                                 None\n",
      "4384                                                 None\n",
      "4385                                                 None\n",
      "4386    William Blair downgraded United Technologies (...\n",
      "4387    View Single CommentphilDecember 13th, 2013 4:5...\n",
      "4388    UTC Chairman & CEO Expects 2013 Earnings Per S...\n",
      "4389                                                 None\n",
      "4390                                                 None\n",
      "4391                                                 None\n",
      "4392                                                 None\n",
      "4393    December 09, 2013 A funny thing happened relea...\n",
      "4394                                                 None\n",
      "4395                                                 None\n",
      "4396    United Technologies industrial conglomerate pr...\n",
      "4397    General Electric one largest industrial conglo...\n",
      "4398                                                 None\n",
      "4399                                                 None\n",
      "4400                  Company Conference Call Transcripts\n",
      "4401                                                 None\n",
      "4402    Based International Monetary Fund's (IMF) glob...\n",
      "4403    December 03, 2013 A popular chart among stock ...\n",
      "4404    United Technologies announced third quarter ea...\n",
      "4405    United Technologies announced third quarter ea...\n",
      "4406                                                 None\n",
      "4407    United Technologies' Commercial business expec...\n",
      "4408    United Technologies' Commercial business expec...\n",
      "4409                                                 None\n",
      "Name: no_stopwords, Length: 4410, dtype: object\n",
      "Stop words are removed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xlwt,json\n",
    "\n",
    "with open('./twits/UTX.txt','r')as file_open:\n",
    "    data=json.load(file_open)\n",
    "df = pd.DataFrame(data)\n",
    "english=[]\n",
    "with open('./english','r')as file:\n",
    "    for line in file:\n",
    "        english.append(line.strip('\\n'))\n",
    "df = pd.DataFrame(data)\n",
    "def extract_stopwords(x):\n",
    "    if len(x) == 0:\n",
    "        return None\n",
    "    description = x[0]['description']\n",
    "    if description:\n",
    "        return ' '.join([word for word in description.split() if word not in english])\n",
    "    else:\n",
    "        return None\n",
    "df['no_stopwords'] = df['links'].apply(lambda x: extract_stopwords(x))\n",
    "print(df['no_stopwords'])\n",
    "print('Stop words are removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.c.iii Remove company names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company names are removed\n"
     ]
    }
   ],
   "source": [
    "def find_out(x):\n",
    "    if x:\n",
    "        if '$UTX' in x:\n",
    "            x = x.replace('$UTX', '')\n",
    "        if 'UTX' in x:\n",
    "            x = x.replace('UTX', '')\n",
    "        return x\n",
    "    else:\n",
    "        return None\n",
    "df['rm_comp_names'] = df['no_stopwords'].apply(lambda x: find_out(x))\n",
    "print('Company names are removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.c.iv Remove posts mentioning/tagging multiple stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Fluid. Too fluid. It's difficult make informed...\n",
      "1       Donald Trump's Carrier Air Conditioner deal, l...\n",
      "2       Donald Trump, \"Move manufacturing offshore, fa...\n",
      "3                                                    None\n",
      "4       Earnings estimates  thousands professional ama...\n",
      "5       Management one important areas company. We loo...\n",
      "6                                                    None\n",
      "7       News Corp network leading companies worlds div...\n",
      "8                                                    None\n",
      "9       5 Trade ideas excerpted detailed analysis plan...\n",
      "10                                                   None\n",
      "11                                                   None\n",
      "12      Capital Goods stocks experienced great Novembe...\n",
      "13      Ford Motor Co. forge ahead shifting small-car ...\n",
      "14                                                   None\n",
      "15      It's Fix The Tax Code Friday! Earlier week, ai...\n",
      "16                                                   None\n",
      "17                                                   None\n",
      "18                                                   None\n",
      "19                                                   None\n",
      "20      John Bassett, CEO, Vaughan-Bassett Furniture C...\n",
      "21      Earnings estimates  thousands professional ama...\n",
      "22                                                   None\n",
      "23                                                   None\n",
      "24                                                   None\n",
      "25                                                   None\n",
      "26                                                   None\n",
      "27      News Corp network leading companies worlds div...\n",
      "28      From Yahoo Singapore Finance: Dec 2 (Reuters) ...\n",
      "29      Timothy A. Clary | AFP | Getty Images It's bad...\n",
      "                              ...                        \n",
      "4380                                                 None\n",
      "4381                                                 None\n",
      "4382                                                 None\n",
      "4383                                                 None\n",
      "4384                                                 None\n",
      "4385                                                 None\n",
      "4386                                                 None\n",
      "4387    View Single CommentphilDecember 13th, 2013 4:5...\n",
      "4388                                                 None\n",
      "4389                                                 None\n",
      "4390                                                 None\n",
      "4391                                                 None\n",
      "4392                                                 None\n",
      "4393    December 09, 2013 A funny thing happened relea...\n",
      "4394                                                 None\n",
      "4395                                                 None\n",
      "4396                                                 None\n",
      "4397    General Electric one largest industrial conglo...\n",
      "4398                                                 None\n",
      "4399                                                 None\n",
      "4400                  Company Conference Call Transcripts\n",
      "4401                                                 None\n",
      "4402    Based International Monetary Fund's (IMF) glob...\n",
      "4403    December 03, 2013 A popular chart among stock ...\n",
      "4404                                                 None\n",
      "4405                                                 None\n",
      "4406                                                 None\n",
      "4407    United Technologies' Commercial business expec...\n",
      "4408    United Technologies' Commercial business expec...\n",
      "4409                                                 None\n",
      "Name: rm_mul_stocks, Length: 4410, dtype: object\n",
      "Posts mentioning/tagging multiple stocks are removed\n"
     ]
    }
   ],
   "source": [
    "def mul_stock(x):\n",
    "    if x:\n",
    "        if '$' in x:\n",
    "            return None\n",
    "        else:\n",
    "            return x\n",
    "    else:\n",
    "        return None\n",
    "df['rm_mul_stocks'] = df['rm_comp_names'].apply(lambda x: mul_stock(x))\n",
    "print(df['rm_mul_stocks'])\n",
    "print('Posts mentioning/tagging multiple stocks are removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.c.v Aggregate posts by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>classes</th>\n",
       "      <th>conversation</th>\n",
       "      <th>created_at</th>\n",
       "      <th>direct</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to_message_id</th>\n",
       "      <th>investor_relations</th>\n",
       "      <th>link_embed</th>\n",
       "      <th>links</th>\n",
       "      <th>...</th>\n",
       "      <th>structurable</th>\n",
       "      <th>total_likes</th>\n",
       "      <th>total_reshares</th>\n",
       "      <th>user</th>\n",
       "      <th>view_chart</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>rm_comp_names</th>\n",
       "      <th>rm_mul_stocks</th>\n",
       "      <th>day</th>\n",
       "      <th>rm_none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C, WFC, UTX: Jim Cramer's Views $UTX http://dl...</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon, 05 Dec 2016 06:06:09 -0000</td>\n",
       "      <td>False</td>\n",
       "      <td>68470280</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'provider_url': 'https://www.thestreet.com', ...</td>\n",
       "      <td>[{'provider_url': 'https://www.thestreet.com',...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'username': 'TheStreet', 'id': 16412, 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fluid. Too fluid. It's difficult make informed...</td>\n",
       "      <td>Fluid. Too fluid. It's difficult make informed...</td>\n",
       "      <td>Fluid. Too fluid. It's difficult make informed...</td>\n",
       "      <td>20161205</td>\n",
       "      <td>fluid. too fluid. it's difficult make informed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump's Carrier Deal, Banks,Technology And Fre...</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun, 04 Dec 2016 23:23:58 -0000</td>\n",
       "      <td>False</td>\n",
       "      <td>68457987</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'provider_url': 'http://www.talkmarkets.com',...</td>\n",
       "      <td>[{'provider_url': 'http://www.talkmarkets.com'...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'username': 'GaryAnderson', 'id': 586691, 'na...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Donald Trump's Carrier Air Conditioner deal, l...</td>\n",
       "      <td>Donald Trump's Carrier Air Conditioner deal, l...</td>\n",
       "      <td>Donald Trump's Carrier Air Conditioner deal, l...</td>\n",
       "      <td>20161204</td>\n",
       "      <td>donald trump's carrier air conditioner deal, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump, “Move manufacturing offshore, fa...</td>\n",
       "      <td>default suggested</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun, 04 Dec 2016 22:33:27 -0000</td>\n",
       "      <td>False</td>\n",
       "      <td>68456164</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'provider_url': 'http://www.livetradingnews....</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'username': 'paulebeling', 'id': 655752, 'nam...</td>\n",
       "      <td>{'path': '/message/68456164#68456164', 'image'...</td>\n",
       "      <td>Donald Trump, \"Move manufacturing offshore, fa...</td>\n",
       "      <td>Donald Trump, \"Move manufacturing offshore, fa...</td>\n",
       "      <td>Donald Trump, \"Move manufacturing offshore, fa...</td>\n",
       "      <td>20161204</td>\n",
       "      <td>donald trump, \"move manufacturing offshore, fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Estimize EPS expectations are 0.53% higher tha...</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun, 04 Dec 2016 19:49:16 -0000</td>\n",
       "      <td>False</td>\n",
       "      <td>68452567</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'provider_url': 'https://www.estimize.com', ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'username': 'EstimizeAlerts', 'id': 727510, '...</td>\n",
       "      <td>{'path': '/message/68452567#68452567', 'image'...</td>\n",
       "      <td>Earnings estimates UTX thousands professional ...</td>\n",
       "      <td>Earnings estimates  thousands professional ama...</td>\n",
       "      <td>Earnings estimates  thousands professional ama...</td>\n",
       "      <td>20161204</td>\n",
       "      <td>earnings estimates  thousands professional ama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>70.2% earnings growth in 3 years.. What\u0019s your...</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun, 04 Dec 2016 19:47:23 -0000</td>\n",
       "      <td>False</td>\n",
       "      <td>68452526</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'provider_url': 'https://simplywall.st', 'de...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'username': 'WarrenofWallSt', 'id': 479001, '...</td>\n",
       "      <td>{'path': '/message/68452526#68452526', 'image'...</td>\n",
       "      <td>Management one important areas company. We loo...</td>\n",
       "      <td>Management one important areas company. We loo...</td>\n",
       "      <td>Management one important areas company. We loo...</td>\n",
       "      <td>20161204</td>\n",
       "      <td>management one important areas company. we loo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body            classes  \\\n",
       "0  C, WFC, UTX: Jim Cramer's Views $UTX http://dl...            default   \n",
       "1  Trump's Carrier Deal, Banks,Technology And Fre...            default   \n",
       "2  Donald Trump, “Move manufacturing offshore, fa...  default suggested   \n",
       "4  Estimize EPS expectations are 0.53% higher tha...            default   \n",
       "5  70.2% earnings growth in 3 years.. What\u0019s your...            default   \n",
       "\n",
       "  conversation                       created_at  direct        id  \\\n",
       "0          NaN  Mon, 05 Dec 2016 06:06:09 -0000   False  68470280   \n",
       "1          NaN  Sun, 04 Dec 2016 23:23:58 -0000   False  68457987   \n",
       "2          NaN  Sun, 04 Dec 2016 22:33:27 -0000   False  68456164   \n",
       "4          NaN  Sun, 04 Dec 2016 19:49:16 -0000   False  68452567   \n",
       "5          NaN  Sun, 04 Dec 2016 19:47:23 -0000   False  68452526   \n",
       "\n",
       "  in_reply_to_message_id  investor_relations  \\\n",
       "0                   None               False   \n",
       "1                   None               False   \n",
       "2                   None               False   \n",
       "4                   None               False   \n",
       "5                   None               False   \n",
       "\n",
       "                                          link_embed  \\\n",
       "0  {'provider_url': 'https://www.thestreet.com', ...   \n",
       "1  {'provider_url': 'http://www.talkmarkets.com',...   \n",
       "2                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "\n",
       "                                               links  \\\n",
       "0  [{'provider_url': 'https://www.thestreet.com',...   \n",
       "1  [{'provider_url': 'http://www.talkmarkets.com'...   \n",
       "2  [{'provider_url': 'http://www.livetradingnews....   \n",
       "4  [{'provider_url': 'https://www.estimize.com', ...   \n",
       "5  [{'provider_url': 'https://simplywall.st', 'de...   \n",
       "\n",
       "                         ...                         structurable total_likes  \\\n",
       "0                        ...                                  NaN           0   \n",
       "1                        ...                                  NaN           0   \n",
       "2                        ...                                  NaN           0   \n",
       "4                        ...                                  NaN           1   \n",
       "5                        ...                                  NaN           0   \n",
       "\n",
       "  total_reshares                                               user  \\\n",
       "0              0  {'username': 'TheStreet', 'id': 16412, 'name':...   \n",
       "1              0  {'username': 'GaryAnderson', 'id': 586691, 'na...   \n",
       "2              0  {'username': 'paulebeling', 'id': 655752, 'nam...   \n",
       "4              1  {'username': 'EstimizeAlerts', 'id': 727510, '...   \n",
       "5              0  {'username': 'WarrenofWallSt', 'id': 479001, '...   \n",
       "\n",
       "                                          view_chart  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  {'path': '/message/68456164#68456164', 'image'...   \n",
       "4  {'path': '/message/68452567#68452567', 'image'...   \n",
       "5  {'path': '/message/68452526#68452526', 'image'...   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "0  Fluid. Too fluid. It's difficult make informed...   \n",
       "1  Donald Trump's Carrier Air Conditioner deal, l...   \n",
       "2  Donald Trump, \"Move manufacturing offshore, fa...   \n",
       "4  Earnings estimates UTX thousands professional ...   \n",
       "5  Management one important areas company. We loo...   \n",
       "\n",
       "                                       rm_comp_names  \\\n",
       "0  Fluid. Too fluid. It's difficult make informed...   \n",
       "1  Donald Trump's Carrier Air Conditioner deal, l...   \n",
       "2  Donald Trump, \"Move manufacturing offshore, fa...   \n",
       "4  Earnings estimates  thousands professional ama...   \n",
       "5  Management one important areas company. We loo...   \n",
       "\n",
       "                                       rm_mul_stocks       day  \\\n",
       "0  Fluid. Too fluid. It's difficult make informed...  20161205   \n",
       "1  Donald Trump's Carrier Air Conditioner deal, l...  20161204   \n",
       "2  Donald Trump, \"Move manufacturing offshore, fa...  20161204   \n",
       "4  Earnings estimates  thousands professional ama...  20161204   \n",
       "5  Management one important areas company. We loo...  20161204   \n",
       "\n",
       "                                             rm_none  \n",
       "0  fluid. too fluid. it's difficult make informed...  \n",
       "1  donald trump's carrier air conditioner deal, l...  \n",
       "2  donald trump, \"move manufacturing offshore, fa...  \n",
       "4  earnings estimates  thousands professional ama...  \n",
       "5  management one important areas company. we loo...  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_month = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
    "              'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}\n",
    "df['day'] = df['created_at'].apply(lambda x: int(x.split(',')[1].split(' ')[1]) + dict_month[x.split(',')[1].split(' ')[2]] * 100 + int(x.split(',')[1].split(' ')[3]) * 10000)\n",
    "#df = df.sort_values(by = \"day\",ascending= True) \n",
    "df_posts = df\n",
    "def rm_none(val):\n",
    "    res = [] \n",
    "    if val != None:\n",
    "        res.append(val)\n",
    "    return '.'.join(res)\n",
    "df_posts['rm_none'] = df_posts['rm_mul_stocks'].apply(lambda x: rm_none(x))\n",
    "df_posts['rm_none'] = df_posts['rm_none'].apply(lambda x: x.lower())\n",
    "df_posts = df_posts.loc[df_posts['rm_none'] != '', :]\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1907, 28)\n"
     ]
    }
   ],
   "source": [
    "#df_posts = df_posts.sort_values(by = \"day\",ascending= True) \n",
    "#df_posts=df_posts.groupby(['day'], as_index = False)\n",
    "print(df_posts.shape)\n",
    "df_posts=df_posts.reindex([i for i in range(len(df_posts))])\n",
    "train = df_posts.loc[: int(df_posts.shape[0] * 0.7) , ['day', 'rm_none']]\n",
    "test = df_posts.loc[int(df_posts.shape[0] * 0.7): , ['day', 'rm_none']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.d.i  Calculate the frequencies of the words in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   0    1\n",
      "0            company  315\n",
      "1             united  305\n",
      "2       technologies  287\n",
      "3                ceo  235\n",
      "4           earnings  221\n",
      "5              stock  187\n",
      "6            trading  184\n",
      "7                the  170\n",
      "8                one  147\n",
      "9          estimates  136\n",
      "10           insider  126\n",
      "11           gregory  119\n",
      "12                we  118\n",
      "13            around  117\n",
      "14              long  114\n",
      "15              look  114\n",
      "16         important  113\n",
      "17              data  113\n",
      "18      professional  112\n",
      "19        management  112\n",
      "20             board  111\n",
      "21              open  110\n",
      "22              nyse  110\n",
      "23         directors  109\n",
      "24                 j  109\n",
      "25                mr  108\n",
      "26        conference  108\n",
      "27          analysts  107\n",
      "28      compensation  105\n",
      "29              team  105\n",
      "...              ...  ...\n",
      "3727      nasdaqaapl    1\n",
      "3728        outlined    1\n",
      "3729      opposition    1\n",
      "3730    surveillance    1\n",
      "3731         justify    1\n",
      "3732           users    1\n",
      "3733           cisco    1\n",
      "3734      nasdaqcsco    1\n",
      "3735     anniversary    1\n",
      "3736           india    1\n",
      "3737  conceptualized    1\n",
      "3738          claims    1\n",
      "3739       providers    1\n",
      "3740        reinvent    1\n",
      "3741           built    1\n",
      "3742        reaching    1\n",
      "3743           bulls    1\n",
      "3744           soon?    1\n",
      "3745       safeguard    1\n",
      "3746         nations    1\n",
      "3747         weapons    1\n",
      "3748         exports    1\n",
      "3749      benefiting    1\n",
      "3750      nasdaqgpro    1\n",
      "3751          jumped    1\n",
      "3752            gpro    1\n",
      "3753             ios    1\n",
      "3754         cameras    1\n",
      "3755        controls    1\n",
      "3756           wrist    1\n",
      "\n",
      "[3757 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "punc = '[,.!%()+\\'\\d:-]'\n",
    "word_list=[]\n",
    "for item in df_posts['rm_none']:\n",
    "    #print(item.strip('.').split(' '))\n",
    "    try:\n",
    "        new=re.sub(punc, '', item)\n",
    "        for content in new.split(' '):\n",
    "            word_list.append(content)\n",
    "    except:\n",
    "        continue\n",
    "#print(word_list)\n",
    "\n",
    "#Statistical word frequency\n",
    "word_count_dict = {}\n",
    "for word in word_list:\n",
    "    if word=='':\n",
    "        continue\n",
    "    if word in word_count_dict.keys():\n",
    "        word_count_dict[word] += 1\n",
    "    else:\n",
    "        word_count_dict[word] =1\n",
    "word_count_dict = sorted(word_count_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "# word_count_dict = nltk.FreqDist(word_list)\n",
    "\n",
    "#convert to DataFrame\n",
    "word_fre = pd.DataFrame(word_count_dict)\n",
    "print(word_fre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.d.ii Keep words that occured at least 25 times in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>company</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>united</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>technologies</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ceo</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>earnings</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0    1\n",
       "0       company  315\n",
       "1        united  305\n",
       "2  technologies  287\n",
       "3           ceo  235\n",
       "4      earnings  221"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_fre = word_fre.loc[word_fre.iloc[:, 1] >= 25, :]\n",
    "word_fre.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.d.iii Calculate the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>classes</th>\n",
       "      <th>conversation</th>\n",
       "      <th>created_at</th>\n",
       "      <th>direct</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to_message_id</th>\n",
       "      <th>investor_relations</th>\n",
       "      <th>link_embed</th>\n",
       "      <th>links</th>\n",
       "      <th>...</th>\n",
       "      <th>total_reshares</th>\n",
       "      <th>user</th>\n",
       "      <th>view_chart</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>rm_comp_names</th>\n",
       "      <th>rm_mul_stocks</th>\n",
       "      <th>day</th>\n",
       "      <th>rm_none</th>\n",
       "      <th>tf</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C, WFC, UTX: Jim Cramer's Views $UTX http://dl...</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon, 05 Dec 2016 06:06:09 -0000</td>\n",
       "      <td>False</td>\n",
       "      <td>68470280.0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'provider_url': 'https://www.thestreet.com', ...</td>\n",
       "      <td>[{'provider_url': 'https://www.thestreet.com',...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'username': 'TheStreet', 'id': 16412, 'name':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fluid. Too fluid. It's difficult make informed...</td>\n",
       "      <td>Fluid. Too fluid. It's difficult make informed...</td>\n",
       "      <td>Fluid. Too fluid. It's difficult make informed...</td>\n",
       "      <td>20161205.0</td>\n",
       "      <td>fluid. too fluid. it's difficult make informed...</td>\n",
       "      <td>[0.0043859649122807015, 0.0043859649122807015,...</td>\n",
       "      <td>[0.012956316996221769, 0.01404752057117099, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump's Carrier Deal, Banks,Technology And Fre...</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun, 04 Dec 2016 23:23:58 -0000</td>\n",
       "      <td>False</td>\n",
       "      <td>68457987.0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'provider_url': 'http://www.talkmarkets.com',...</td>\n",
       "      <td>[{'provider_url': 'http://www.talkmarkets.com'...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'username': 'GaryAnderson', 'id': 586691, 'na...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Donald Trump's Carrier Air Conditioner deal, l...</td>\n",
       "      <td>Donald Trump's Carrier Air Conditioner deal, l...</td>\n",
       "      <td>Donald Trump's Carrier Air Conditioner deal, l...</td>\n",
       "      <td>20161204.0</td>\n",
       "      <td>donald trump's carrier air conditioner deal, l...</td>\n",
       "      <td>[0.003401360544217687, 0.003401360544217687, 0...</td>\n",
       "      <td>[0.01004775603788627, 0.010893995544989748, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump, “Move manufacturing offshore, fa...</td>\n",
       "      <td>default suggested</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun, 04 Dec 2016 22:33:27 -0000</td>\n",
       "      <td>False</td>\n",
       "      <td>68456164.0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'provider_url': 'http://www.livetradingnews....</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'username': 'paulebeling', 'id': 655752, 'nam...</td>\n",
       "      <td>{'path': '/message/68456164#68456164', 'image'...</td>\n",
       "      <td>Donald Trump, \"Move manufacturing offshore, fa...</td>\n",
       "      <td>Donald Trump, \"Move manufacturing offshore, fa...</td>\n",
       "      <td>Donald Trump, \"Move manufacturing offshore, fa...</td>\n",
       "      <td>20161204.0</td>\n",
       "      <td>donald trump, \"move manufacturing offshore, fa...</td>\n",
       "      <td>[0.0019230769230769232, 0.0019230769230769232,...</td>\n",
       "      <td>[0.005680846682958776, 0.006159297481205743, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.16666666666666666, 0.16666666666666666, 0.1...</td>\n",
       "      <td>[0.4923400458564272, 0.5338057817044977, 0.540...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Estimize EPS expectations are 0.53% higher tha...</td>\n",
       "      <td>default</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun, 04 Dec 2016 19:49:16 -0000</td>\n",
       "      <td>False</td>\n",
       "      <td>68452567.0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'provider_url': 'https://www.estimize.com', ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'username': 'EstimizeAlerts', 'id': 727510, '...</td>\n",
       "      <td>{'path': '/message/68452567#68452567', 'image'...</td>\n",
       "      <td>Earnings estimates UTX thousands professional ...</td>\n",
       "      <td>Earnings estimates  thousands professional ama...</td>\n",
       "      <td>Earnings estimates  thousands professional ama...</td>\n",
       "      <td>20161204.0</td>\n",
       "      <td>earnings estimates  thousands professional ama...</td>\n",
       "      <td>[0.005208333333333333, 0.005208333333333333, 0...</td>\n",
       "      <td>[0.01538562643301335, 0.016681430678265552, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body            classes  \\\n",
       "0  C, WFC, UTX: Jim Cramer's Views $UTX http://dl...            default   \n",
       "1  Trump's Carrier Deal, Banks,Technology And Fre...            default   \n",
       "2  Donald Trump, “Move manufacturing offshore, fa...  default suggested   \n",
       "3                                                NaN                NaN   \n",
       "4  Estimize EPS expectations are 0.53% higher tha...            default   \n",
       "\n",
       "  conversation                       created_at direct          id  \\\n",
       "0          NaN  Mon, 05 Dec 2016 06:06:09 -0000  False  68470280.0   \n",
       "1          NaN  Sun, 04 Dec 2016 23:23:58 -0000  False  68457987.0   \n",
       "2          NaN  Sun, 04 Dec 2016 22:33:27 -0000  False  68456164.0   \n",
       "3          NaN                              NaN    NaN         NaN   \n",
       "4          NaN  Sun, 04 Dec 2016 19:49:16 -0000  False  68452567.0   \n",
       "\n",
       "  in_reply_to_message_id investor_relations  \\\n",
       "0                   None              False   \n",
       "1                   None              False   \n",
       "2                   None              False   \n",
       "3                    NaN                NaN   \n",
       "4                   None              False   \n",
       "\n",
       "                                          link_embed  \\\n",
       "0  {'provider_url': 'https://www.thestreet.com', ...   \n",
       "1  {'provider_url': 'http://www.talkmarkets.com',...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                               links  \\\n",
       "0  [{'provider_url': 'https://www.thestreet.com',...   \n",
       "1  [{'provider_url': 'http://www.talkmarkets.com'...   \n",
       "2  [{'provider_url': 'http://www.livetradingnews....   \n",
       "3                                                NaN   \n",
       "4  [{'provider_url': 'https://www.estimize.com', ...   \n",
       "\n",
       "                         ...                         total_reshares  \\\n",
       "0                        ...                                    0.0   \n",
       "1                        ...                                    0.0   \n",
       "2                        ...                                    0.0   \n",
       "3                        ...                                    NaN   \n",
       "4                        ...                                    1.0   \n",
       "\n",
       "                                                user  \\\n",
       "0  {'username': 'TheStreet', 'id': 16412, 'name':...   \n",
       "1  {'username': 'GaryAnderson', 'id': 586691, 'na...   \n",
       "2  {'username': 'paulebeling', 'id': 655752, 'nam...   \n",
       "3                                                NaN   \n",
       "4  {'username': 'EstimizeAlerts', 'id': 727510, '...   \n",
       "\n",
       "                                          view_chart  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  {'path': '/message/68456164#68456164', 'image'...   \n",
       "3                                                NaN   \n",
       "4  {'path': '/message/68452567#68452567', 'image'...   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "0  Fluid. Too fluid. It's difficult make informed...   \n",
       "1  Donald Trump's Carrier Air Conditioner deal, l...   \n",
       "2  Donald Trump, \"Move manufacturing offshore, fa...   \n",
       "3                                                NaN   \n",
       "4  Earnings estimates UTX thousands professional ...   \n",
       "\n",
       "                                       rm_comp_names  \\\n",
       "0  Fluid. Too fluid. It's difficult make informed...   \n",
       "1  Donald Trump's Carrier Air Conditioner deal, l...   \n",
       "2  Donald Trump, \"Move manufacturing offshore, fa...   \n",
       "3                                                NaN   \n",
       "4  Earnings estimates  thousands professional ama...   \n",
       "\n",
       "                                       rm_mul_stocks         day  \\\n",
       "0  Fluid. Too fluid. It's difficult make informed...  20161205.0   \n",
       "1  Donald Trump's Carrier Air Conditioner deal, l...  20161204.0   \n",
       "2  Donald Trump, \"Move manufacturing offshore, fa...  20161204.0   \n",
       "3                                                NaN         NaN   \n",
       "4  Earnings estimates  thousands professional ama...  20161204.0   \n",
       "\n",
       "                                             rm_none  \\\n",
       "0  fluid. too fluid. it's difficult make informed...   \n",
       "1  donald trump's carrier air conditioner deal, l...   \n",
       "2  donald trump, \"move manufacturing offshore, fa...   \n",
       "3                                                NaN   \n",
       "4  earnings estimates  thousands professional ama...   \n",
       "\n",
       "                                                  tf  \\\n",
       "0  [0.0043859649122807015, 0.0043859649122807015,...   \n",
       "1  [0.003401360544217687, 0.003401360544217687, 0...   \n",
       "2  [0.0019230769230769232, 0.0019230769230769232,...   \n",
       "3  [0.16666666666666666, 0.16666666666666666, 0.1...   \n",
       "4  [0.005208333333333333, 0.005208333333333333, 0...   \n",
       "\n",
       "                                              tf-idf  \n",
       "0  [0.012956316996221769, 0.01404752057117099, 0....  \n",
       "1  [0.01004775603788627, 0.010893995544989748, 0....  \n",
       "2  [0.005680846682958776, 0.006159297481205743, 0...  \n",
       "3  [0.4923400458564272, 0.5338057817044977, 0.540...  \n",
       "4  [0.01538562643301335, 0.016681430678265552, 0....  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#idf\n",
    "N = df_posts.shape[0]\n",
    "def count_nx(x):\n",
    "    count = 0\n",
    "    for values in df_posts['rm_none']:\n",
    "        try:    \n",
    "            if x in values:\n",
    "                count += 1\n",
    "        except:\n",
    "            continue\n",
    "    return count\n",
    "word_fre['N_x'] = word_fre[0].apply(lambda x: count_nx(x))\n",
    "\n",
    "word_fre = word_fre.loc[(word_fre['N_x'] > 0) & (word_fre['N_x'] <= word_fre[1]), :]\n",
    "import math\n",
    "word_fre['idf'] = word_fre['N_x'].apply(lambda x: math.log((N + 1) / x + 1) + 1)\n",
    "#tf\n",
    "import collections\n",
    "def cal_tf(x):\n",
    "    word_list=[]\n",
    "    for item in str(x):\n",
    "        if str(item)!='nan':\n",
    "            word_list += item.split()\n",
    "    nums = collections.Counter(word_list)\n",
    "    tol_words = len(word_list)\n",
    "    \n",
    "    sel_words = word_fre[0].values\n",
    "    tf_list = []\n",
    "    \n",
    "    for i in sel_words:\n",
    "        tf_list.append((nums[i] + 1) / (2 * tol_words))\n",
    "        \n",
    "    return tf_list\n",
    "df_posts['tf'] = df_posts['rm_none'].apply(lambda x: cal_tf(x))\n",
    "\n",
    "#tf-idf\n",
    "import numpy as np\n",
    "def cal_tfidf(x):\n",
    "    return (np.array(x) * np.array(word_fre['idf']))\n",
    "df_posts['tf-idf'] = df_posts['tf'].apply(lambda x: cal_tfidf(x))\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.e chi-squared test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [0.012956316996221769, 0.01404752057117099, 0....\n",
      "1       [0.01004775603788627, 0.010893995544989748, 0....\n",
      "2       [0.005680846682958776, 0.006159297481205743, 0...\n",
      "3       [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "4       [0.01538562643301335, 0.016681430678265552, 0....\n",
      "5       [0.011105414568190088, 0.012040731918146563, 0...\n",
      "6       [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "7       [0.004907043646409574, 0.005320323405692668, 0...\n",
      "8       [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "9       [0.009529162177866334, 0.010331724807183825, 0...\n",
      "10      [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "11      [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "12      [0.005792235833605027, 0.006280068020052914, 0...\n",
      "13      [0.00997986579438704, 0.01082038746698306, 0.0...\n",
      "14      [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "15      [0.005332202662704988, 0.0057812900545613465, ...\n",
      "16      [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "17      [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "18      [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "19      [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "20      [0.015547580395466123, 0.01685702468540519, 0....\n",
      "21      [0.01538562643301335, 0.016681430678265552, 0....\n",
      "22      [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "23      [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "24      [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "25      [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "26      [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "27      [0.005792235833605027, 0.006280068020052914, 0...\n",
      "28      [0.00928943482747976, 0.010071807201971655, 0....\n",
      "29      [0.00928943482747976, 0.010071807201971655, 0....\n",
      "                              ...                        \n",
      "1877    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1878    [0.007613505863759184, 0.008254728583059243, 0...\n",
      "1879    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1880    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1881    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1882    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1883    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1884    [0.00811549526136968, 0.00879899640172249, 0.0...\n",
      "1885    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1886    [0.021100287679561167, 0.02287739064447847, 0....\n",
      "1887    [0.012624103739908393, 0.013687327736012762, 0...\n",
      "1888    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1889    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1890    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1891    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1892    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1893    [0.0059318077814027375, 0.0064313949602951516,...\n",
      "1894    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1895    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1896    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1897    [0.004923400458564273, 0.005338057817044977, 0...\n",
      "1898    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1899    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1900    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1901    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1902    [0.008791786533150487, 0.009532246101866029, 0...\n",
      "1903    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1904    [0.007240294792006283, 0.007850085025066142, 0...\n",
      "1905    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "1906    [0.4923400458564272, 0.5338057817044977, 0.540...\n",
      "Name: new_tfidf, Length: 1907, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "list_tfidf = []\n",
    "for i in df_posts.index:\n",
    "    list_tfidf.append(df_posts['tf-idf'][i])\n",
    "features = np.array(list_tfidf)\n",
    "chisq, p = scipy.stats.chisquare(features)\n",
    "top=100\n",
    "def find_top100(x):\n",
    "    top100_list = []\n",
    "    top_index = chisq.argsort()[-top: ]\n",
    "    for i in top_index:\n",
    "        top100_list.append(x[i])\n",
    "    return np.array(top100_list)\n",
    "df_posts['new_tfidf'] = df_posts['tf-idf'].apply(lambda x: find_top100(x))\n",
    "print(df_posts['new_tfidf'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:2540: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "#preprocessing\n",
    "p_data_5 = p_data.loc[: p_data.shape[0] - 6, :]\n",
    "date_length = p_data_5.shape[0]\n",
    "def find_return(x):\n",
    "    for i in range(date_length):\n",
    "        year, month, day = p_data_5.loc[i, 'Date'].split('-')\n",
    "        number = int(year) * 10000 + int(month) * 100 + int(day)\n",
    "        if number == x:\n",
    "            return 1\n",
    "#     print(x)\n",
    "    return 0\n",
    "df_posts['return_sign'] = df_posts['day'].apply(lambda x: find_return(x))\n",
    "com_df = df_posts.loc[df_posts['return_sign'] != 0, :]\n",
    "\n",
    "#assgin labels\n",
    "def ass_labels(x):\n",
    "    for i in range(date_length):\n",
    "        year, month, day = p_data_5.loc[i, 'Date'].split('-')\n",
    "        number = int(year) * 10000 + int(month) * 100 + int(day)\n",
    "        if number == x:\n",
    "            rt_3 = p_data_5.loc[i, 'rt_3']\n",
    "            rt_5 = p_data_5.loc[i, 'rt_5']\n",
    "            if rt_3 > 0 and rt_5 > 0:\n",
    "                return 1, 1\n",
    "            elif rt_3 > 0 and rt_5 <= 0:\n",
    "                return 1, 0\n",
    "            elif rt_3 <= 0 and rt_5 > 0:\n",
    "                return 0, 1\n",
    "            else:\n",
    "                return 0, 0\n",
    "com_df[['rt_3', 'rt_5']] = com_df['day'].apply(lambda x: pd.Series(ass_labels(x)))\n",
    "\n",
    "#festures:\n",
    "# com_df['tf-idf']\n",
    "#labels\n",
    "#com_df['rt_3'], com_df['rt_5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.f.ii Naive Bayes Binary Classiﬁer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy of rt(3):\n",
      "0.5452898550724637\n",
      "test accuracy of rt(3):\n",
      "0.3881856540084388\n",
      "train accuracy of rt(5):\n",
      "0.5108695652173914\n",
      "test accuracy of rt(5):\n",
      "0.43037974683544306\n",
      "confusion matrix for rt(3) traning data:\n",
      "[[245  46]\n",
      " [205  56]]\n",
      "confusion matrix for rt(3) testing data:\n",
      "[[ 64  15]\n",
      " [130  28]]\n",
      "confusion matrix for rt(5) traning data:\n",
      "[[221  41]\n",
      " [229  61]]\n",
      "confusion matrix for rt(5) testing data:\n",
      "[[ 70  11]\n",
      " [124  32]]\n",
      "AUC of rt(3) training data:\n",
      "0.5282418927993048\n",
      "precision, recall, and F1-scores of rt(3) training data:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.84      0.66       291\n",
      "          1       0.55      0.21      0.31       261\n",
      "\n",
      "avg / total       0.55      0.55      0.49       552\n",
      "\n",
      "AUC of rt(3) testing data:\n",
      "0.4936708860759494\n",
      "precision, recall, and F1-scores of rt(3) training data:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.33      0.81      0.47        79\n",
      "          1       0.65      0.18      0.28       158\n",
      "\n",
      "avg / total       0.54      0.39      0.34       237\n",
      "\n",
      "AUC of rt(5) training data:\n",
      "0.5269281389839431\n",
      "precision, recall, and F1-scores of rt(5) training data:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.84      0.62       262\n",
      "          1       0.60      0.21      0.31       290\n",
      "\n",
      "avg / total       0.55      0.51      0.46       552\n",
      "\n",
      "AUC of rt(5) testing data:\n",
      "0.5346628679962013\n",
      "precision, recall, and F1-scores of rt(5) training data:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.36      0.86      0.51        81\n",
      "          1       0.74      0.21      0.32       156\n",
      "\n",
      "avg / total       0.61      0.43      0.39       237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get features\n",
    "cla_data = com_df[['tf-idf', 'new_tfidf', 'rt_3', 'rt_5']].copy()\n",
    "\n",
    "list_tfidf = []\n",
    "for i in cla_data.index:\n",
    "    list_tfidf.append(cla_data['new_tfidf'][i])\n",
    "features = np.array(list_tfidf)\n",
    "#split\n",
    "dl = int(features.shape[0] * 0.7)\n",
    "\n",
    "x_train, x_test = features[0: dl - 1], features[dl: ]\n",
    "y_train_rt3, y_test_rt3 = np.array(cla_data.loc[:, 'rt_3'])[0: dl - 1], np.array(cla_data.loc[:, 'rt_3'])[dl: ]\n",
    "y_train_rt5, y_test_rt5 = np.array(cla_data.loc[:, 'rt_5'])[0: dl - 1], np.array(cla_data.loc[:, 'rt_5'])[dl: ]\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "\n",
    "#rt_3\n",
    "gnb.fit(x_train, y_train_rt3)\n",
    "print('train accuracy of rt(3):')\n",
    "print(gnb.score(x_train, y_train_rt3))\n",
    "print('test accuracy of rt(3):')\n",
    "print(gnb.score(x_test, y_test_rt3))\n",
    "\n",
    "#rt_5\n",
    "gnb.fit(x_train, y_train_rt3)\n",
    "print('train accuracy of rt(5):')\n",
    "print(gnb.score(x_train, y_train_rt5))\n",
    "print('test accuracy of rt(5):')\n",
    "print(gnb.score(x_test, y_test_rt5))\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#rt_3\n",
    "gnb.fit(x_train, y_train_rt3)\n",
    "rt3_hat = gnb.predict(x_train)\n",
    "print('confusion matrix for rt(3) traning data:')\n",
    "print(confusion_matrix(y_train_rt3, rt3_hat))\n",
    "\n",
    "rt3_test_hat = gnb.predict(x_test)\n",
    "print('confusion matrix for rt(3) testing data:')\n",
    "print(confusion_matrix(y_test_rt3, rt3_test_hat))\n",
    "\n",
    "#rt_5\n",
    "gnb.fit(x_train, y_train_rt5)\n",
    "rt5_hat = gnb.predict(x_train)\n",
    "print('confusion matrix for rt(5) traning data:')\n",
    "print(confusion_matrix(y_train_rt5, rt5_hat))\n",
    "\n",
    "rt5_test_hat = gnb.predict(x_test)\n",
    "print('confusion matrix for rt(5) testing data:')\n",
    "print(confusion_matrix(y_test_rt5, rt5_test_hat))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#rt_3\n",
    "print('AUC of rt(3) training data:')\n",
    "print(roc_auc_score(y_train_rt3, rt3_hat))\n",
    "print('precision, recall, and F1-scores of rt(3) training data:')\n",
    "print(classification_report(y_train_rt3, rt3_hat))\n",
    "\n",
    "print('AUC of rt(3) testing data:')\n",
    "print(roc_auc_score(y_test_rt3, rt3_test_hat))\n",
    "print('precision, recall, and F1-scores of rt(3) training data:')\n",
    "print(classification_report(y_test_rt3, rt3_test_hat))\n",
    "\n",
    "#rt_5\n",
    "print('AUC of rt(5) training data:')\n",
    "print(roc_auc_score(y_train_rt5, rt5_hat))\n",
    "print('precision, recall, and F1-scores of rt(5) training data:')\n",
    "print(classification_report(y_train_rt5, rt5_hat))\n",
    "\n",
    "print('AUC of rt(5) testing data:')\n",
    "print(roc_auc_score(y_test_rt5, rt5_test_hat))\n",
    "print('precision, recall, and F1-scores of rt(5) training data:')\n",
    "print(classification_report(y_test_rt5, rt5_test_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.f.iii Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 5 fold cv, the best param C is 1\n",
      "confusion matrix for rt(3) traning data:\n",
      "[[245  46]\n",
      " [205  56]]\n",
      "confusion matrix for rt(3) testing data:\n",
      "[[ 64  15]\n",
      " [132  26]]\n",
      "AUC of rt(3) training data:\n",
      "0.5282418927993048\n",
      "precision, recall, and F1-scores of rt(3) training data:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.84      0.66       291\n",
      "          1       0.55      0.21      0.31       261\n",
      "\n",
      "avg / total       0.55      0.55      0.49       552\n",
      "\n",
      "AUC of rt(3) testing data:\n",
      "0.4873417721518987\n",
      "precision, recall, and F1-scores of rt(3) training data:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.33      0.81      0.47        79\n",
      "          1       0.63      0.16      0.26       158\n",
      "\n",
      "avg / total       0.53      0.38      0.33       237\n",
      "\n",
      "train accuracy of rt(5):\n",
      "0.5253623188405797\n",
      "test accuracy of rt(5):\n",
      "0.6329113924050633\n",
      "confusion matrix for rt(5) traning data:\n",
      "[[  9 253]\n",
      " [  9 281]]\n",
      "confusion matrix for rt(5) testing data:\n",
      "[[ 11  70]\n",
      " [ 17 139]]\n",
      "AUC of rt(5) training data:\n",
      "0.5016583311397736\n",
      "precision, recall, and F1-scores of rt(5) training data:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.03      0.06       262\n",
      "          1       0.53      0.97      0.68       290\n",
      "\n",
      "avg / total       0.51      0.53      0.39       552\n",
      "\n",
      "AUC of rt(5) testing data:\n",
      "0.5134140550807218\n",
      "precision, recall, and F1-scores of rt(5) training data:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.39      0.14      0.20        81\n",
      "          1       0.67      0.89      0.76       156\n",
      "\n",
      "avg / total       0.57      0.63      0.57       237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def cv_parameter(model_name,data_train,label_train):\n",
    "    from sklearn.grid_search import GridSearchCV\n",
    "    parameters = {'C': [i for i in range(1,10,1)] }\n",
    "    gsearch1 = GridSearchCV(estimator=LogisticRegression(),\n",
    "                        param_grid=parameters, cv=5)\n",
    "    gsearch1.fit(data_train, label_train)\n",
    "    for key in gsearch1.best_params_.keys():\n",
    "        return gsearch1.best_params_[key]\n",
    "\n",
    "alpha1=cv_parameter('Logistic',x_train,y_train_rt5)\n",
    "lr1 = LogisticRegression(C=alpha1)\n",
    "print('After 5 fold cv, the best param C is {}'.format(alpha1))\n",
    "\n",
    "\n",
    "#rt_3\n",
    "lr1.fit(x_train, y_train_rt3)\n",
    "rt3_hat = lr1.predict(x_train)\n",
    "print('confusion matrix for rt(3) traning data:')\n",
    "print(confusion_matrix(y_train_rt3, rt3_hat))\n",
    "\n",
    "rt3_test_hat = lr1.predict(x_test)\n",
    "print('confusion matrix for rt(3) testing data:')\n",
    "print(confusion_matrix(y_test_rt3, rt3_test_hat))\n",
    "\n",
    "#rt_3\n",
    "print('AUC of rt(3) training data:')\n",
    "print(roc_auc_score(y_train_rt3, rt3_hat))\n",
    "print('precision, recall, and F1-scores of rt(3) training data:')\n",
    "print(classification_report(y_train_rt3, rt3_hat))\n",
    "\n",
    "print('AUC of rt(3) testing data:')\n",
    "print(roc_auc_score(y_test_rt3, rt3_test_hat))\n",
    "print('precision, recall, and F1-scores of rt(3) training data:')\n",
    "print(classification_report(y_test_rt3, rt3_test_hat))\n",
    "\n",
    "\n",
    "#rt_5\n",
    "lr1.fit(x_train, y_train_rt5)\n",
    "print('train accuracy of rt(5):')\n",
    "print(lr1.score(x_train, y_train_rt5))\n",
    "print('test accuracy of rt(5):')\n",
    "print(lr1.score(x_test, y_test_rt5))\n",
    "\n",
    "\n",
    "#rt_5\n",
    "lr1.fit(x_train, y_train_rt5)\n",
    "rt5_hat = lr1.predict(x_train)\n",
    "print('confusion matrix for rt(5) traning data:')\n",
    "print(confusion_matrix(y_train_rt5, rt5_hat))\n",
    "\n",
    "rt5_test_hat = lr1.predict(x_test)\n",
    "print('confusion matrix for rt(5) testing data:')\n",
    "print(confusion_matrix(y_test_rt5, rt5_test_hat))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#rt_5\n",
    "print('AUC of rt(5) training data:')\n",
    "print(roc_auc_score(y_train_rt5, rt5_hat))\n",
    "print('precision, recall, and F1-scores of rt(5) training data:')\n",
    "print(classification_report(y_train_rt5, rt5_hat))\n",
    "\n",
    "print('AUC of rt(5) testing data:')\n",
    "print(roc_auc_score(y_test_rt5, rt5_test_hat))\n",
    "print('precision, recall, and F1-scores of rt(5) training data:')\n",
    "print(classification_report(y_test_rt5, rt5_test_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.f.iv Random Forests and Extra Trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data ......\n",
      "Accuracy:  0.7210144927536232\n",
      "Confusion matrix:  [[157 105]\n",
      " [ 49 241]]\n",
      "AUC:  0.7151355619899974\n",
      "Precision:  0.6965317919075145\n",
      "Recall:  0.8310344827586207\n",
      "F1-score:  0.7578616352201258\n",
      "Testing data ......\n",
      "Accuracy:  0.6118143459915611\n",
      "Confusion matrix:  [[ 39  42]\n",
      " [ 50 106]]\n",
      "AUC:  0.5804843304843306\n",
      "Precision:  0.7162162162162162\n",
      "Recall:  0.6794871794871795\n",
      "F1-score:  0.6973684210526315\n",
      "Training data ......\n",
      "Accuracy:  0.7373188405797102\n",
      "Confusion matrix:  [[161 101]\n",
      " [ 44 246]]\n",
      "AUC:  0.7313898394314294\n",
      "Precision:  0.7089337175792507\n",
      "Recall:  0.8482758620689655\n",
      "F1-score:  0.772370486656201\n",
      "Testing data ......\n",
      "Accuracy:  0.620253164556962\n",
      "Confusion matrix:  [[ 41  40]\n",
      " [ 50 106]]\n",
      "AUC:  0.5928300094966762\n",
      "Precision:  0.726027397260274\n",
      "Recall:  0.6794871794871795\n",
      "F1-score:  0.7019867549668874\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def report_function(y_true, y_pred):\n",
    "    print(\"Accuracy: \", accuracy_score(y_true, y_pred))\n",
    "    print(\"Confusion matrix: \", confusion_matrix(y_true, y_pred))\n",
    "    print(\"AUC: \", roc_auc_score(y_true, y_pred))\n",
    "    print(\"Precision: \", precision_score(y_true, y_pred))\n",
    "    print(\"Recall: \", recall_score(y_true, y_pred))\n",
    "    print(\"F1-score: \", f1_score(y_true, y_pred))\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(x_train, np.array(y_train_rt5))\n",
    "y_train_pred = clf.predict(x_train)\n",
    "print(\"Training data ......\")\n",
    "report_function(y_train_rt5, y_train_pred)\n",
    "y_test_pred = clf.predict(x_test)\n",
    "print(\"Testing data ......\")\n",
    "report_function(np.array(y_test_rt5), y_test_pred)\n",
    "\n",
    "clf = ExtraTreesClassifier()\n",
    "clf.fit(x_train, np.array(y_train_rt5))\n",
    "y_train_pred = clf.predict(x_train)\n",
    "print(\"Training data ......\")\n",
    "report_function(np.array(y_train_rt5), y_train_pred)\n",
    "y_test_pred = clf.predict(x_test)\n",
    "print(\"Testing data ......\")\n",
    "report_function(np.array(y_test_rt5), y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.f.v SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best C is 0.5\n",
      "Training data ......\n",
      "Accuracy:  0.5253623188405797\n",
      "Confusion matrix:  [[  0 262]\n",
      " [  0 290]]\n",
      "AUC:  0.5\n",
      "Precision:  0.5253623188405797\n",
      "Recall:  1.0\n",
      "F1-score:  0.6888361045130642\n",
      "Testing data ......\n",
      "Accuracy:  0.6582278481012658\n",
      "Confusion matrix:  [[  0  81]\n",
      " [  0 156]]\n",
      "AUC:  0.5\n",
      "Precision:  0.6582278481012658\n",
      "Recall:  1.0\n",
      "F1-score:  0.7938931297709924\n",
      "confusion matrix for rt(3) traning data:\n",
      "[[286   5]\n",
      " [252   9]]\n",
      "confusion matrix for rt(3) testing data:\n",
      "[[ 79   0]\n",
      " [155   3]]\n",
      "AUC of rt(3) training data:\n",
      "0.5086503140182487\n",
      "precision, recall, and F1-scores of rt(3) training data:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.98      0.69       291\n",
      "          1       0.64      0.03      0.07       261\n",
      "\n",
      "avg / total       0.58      0.53      0.39       552\n",
      "\n",
      "AUC of rt(3) testing data:\n",
      "0.509493670886076\n",
      "precision, recall, and F1-scores of rt(3) training data:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.34      1.00      0.50        79\n",
      "          1       1.00      0.02      0.04       158\n",
      "\n",
      "avg / total       0.78      0.35      0.19       237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "penalties = [0.5, 1.0, 1.5]\n",
    "\n",
    "max_mean_score = 0.0\n",
    "max_penalty = 0.0\n",
    "for penalty in penalties:\n",
    "    clf = svm.LinearSVC(penalty=\"l1\", C=penalty, dual=False)\n",
    "    scores = cross_val_score(clf, x_train, np.array(y_train_rt5), cv=5)\n",
    "    mean_score = scores.mean()\n",
    "    if max_mean_score < mean_score:\n",
    "        max_mean_score = mean_score\n",
    "        max_penalty = penalty\n",
    "print('The best C is {}'.format(max_penalty))\n",
    "clf = svm.LinearSVC(penalty=\"l1\", C=max_penalty, dual=False)\n",
    "clf.fit(x_train, np.array(y_train_rt5))\n",
    "y_train_pred = clf.predict(x_train)\n",
    "print(\"Training data ......\")\n",
    "report_function(np.array(y_train_rt5), y_train_pred)\n",
    "y_test_pred = clf.predict(x_test)\n",
    "print(\"Testing data ......\")\n",
    "report_function(np.array(y_test_rt5), y_test_pred)\n",
    "\n",
    "\n",
    "#rt_3\n",
    "clf.fit(x_train, y_train_rt3)\n",
    "rt3_hat = clf.predict(x_train)\n",
    "print('confusion matrix for rt(3) traning data:')\n",
    "print(confusion_matrix(y_train_rt3, rt3_hat))\n",
    "\n",
    "rt3_test_hat = clf.predict(x_test)\n",
    "print('confusion matrix for rt(3) testing data:')\n",
    "print(confusion_matrix(y_test_rt3, rt3_test_hat))\n",
    "\n",
    "#rt_3\n",
    "print('AUC of rt(3) training data:')\n",
    "print(roc_auc_score(y_train_rt3, rt3_hat))\n",
    "print('precision, recall, and F1-scores of rt(3) training data:')\n",
    "print(classification_report(y_train_rt3, rt3_hat))\n",
    "\n",
    "print('AUC of rt(3) testing data:')\n",
    "print(roc_auc_score(y_test_rt3, rt3_test_hat))\n",
    "print('precision, recall, and F1-scores of rt(3) training data:')\n",
    "print(classification_report(y_test_rt3, rt3_test_hat))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.g.i KNN regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data ......\n",
      "Accuracy:  0.6268115942028986\n",
      "Confusion matrix:  [[133 129]\n",
      " [ 77 213]]\n",
      "AUC:  0.6210581732034747\n",
      "Precision:  0.6228070175438597\n",
      "Recall:  0.7344827586206897\n",
      "F1-score:  0.6740506329113926\n",
      "Testing data ......\n",
      "Accuracy:  0.5569620253164557\n",
      "Confusion matrix:  [[33 48]\n",
      " [57 99]]\n",
      "AUC:  0.521011396011396\n",
      "Precision:  0.673469387755102\n",
      "Recall:  0.6346153846153846\n",
      "F1-score:  0.6534653465346534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "N=[i for i in range(5,31,1)]\n",
    "\n",
    "max_mean_score = 0.0\n",
    "max_penalty = 0.0\n",
    "for penalty in N:\n",
    "    clf = KNeighborsRegressor(n_neighbors=penalty)\n",
    "    scores = cross_val_score(clf, x_train, np.array(y_train_rt5), cv=5)\n",
    "    mean_score = abs(scores.mean())\n",
    "    if max_mean_score < mean_score:\n",
    "        max_mean_score = mean_score\n",
    "        max_penalty = penalty\n",
    "clf = KNeighborsRegressor(n_neighbors=max_penalty)\n",
    "clf.fit(x_train, np.array(y_train_rt5))\n",
    "y_train_pred = clf.predict(x_train)\n",
    "train_r=[]\n",
    "test_r=[]\n",
    "all=0\n",
    "for i in range(len(y_train_pred)):\n",
    "    train_item=0\n",
    "    if abs(y_train_pred[i])>thre:\n",
    "        train_item=1\n",
    "    train_r.append(train_item)\n",
    "print(\"Training data ......\")\n",
    "\n",
    "report_function(np.array(y_train_rt5), train_r)\n",
    "\n",
    "y_test_pred = clf.predict(x_test)\n",
    "for i in range(len(y_test_pred)):\n",
    "    test_item=0\n",
    "    if abs(y_test_pred[i])>thre:\n",
    "        test_item=1\n",
    "    test_r.append(test_item)\n",
    "print(\"Testing data ......\")\n",
    "report_function(np.array(y_test_rt5), test_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.g.ii SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data ......\n",
      "Accuracy:  0.5253623188405797\n",
      "Confusion matrix:  [[  0 262]\n",
      " [  0 290]]\n",
      "AUC:  0.5\n",
      "Precision:  0.5253623188405797\n",
      "Recall:  1.0\n",
      "F1-score:  0.6888361045130642\n",
      "Testing data ......\n",
      "Accuracy:  0.6582278481012658\n",
      "Confusion matrix:  [[  0  81]\n",
      " [  0 156]]\n",
      "AUC:  0.5\n",
      "Precision:  0.6582278481012658\n",
      "Recall:  1.0\n",
      "F1-score:  0.7938931297709924\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import math\n",
    "\n",
    "penalties = [0.5, 1.0, 1.5]\n",
    "\n",
    "max_mean_score = 0.0\n",
    "max_penalty = 0.0\n",
    "thre=0.5\n",
    "for penalty in penalties:\n",
    "    clf = svm.SVR(C=penalty)\n",
    "    scores = cross_val_score(clf, x_train, np.array(y_train_rt5), cv=5)\n",
    "    mean_score = abs(scores.mean())\n",
    "    if max_mean_score < mean_score:\n",
    "        max_mean_score = mean_score\n",
    "        max_penalty = penalty\n",
    "clf = svm.SVR(C=max_penalty)\n",
    "clf.fit(x_train, np.array(y_train_rt5))\n",
    "y_train_pred = clf.predict(x_train)\n",
    "train_r=[]\n",
    "test_r=[]\n",
    "all=0\n",
    "for i in range(len(y_train_pred)):\n",
    "    train_item=0\n",
    "    if abs(y_train_pred[i])>thre:\n",
    "        train_item=1\n",
    "    train_r.append(train_item)\n",
    "print(\"Training data ......\")\n",
    "\n",
    "report_function(np.array(y_train_rt5), train_r)\n",
    "\n",
    "y_test_pred = clf.predict(x_test)\n",
    "for i in range(len(y_test_pred)):\n",
    "    test_item=0\n",
    "    if abs(y_test_pred[i])>thre:\n",
    "        test_item=1\n",
    "    test_r.append(test_item)\n",
    "print(\"Testing data ......\")\n",
    "report_function(np.array(y_test_rt5), test_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.g.iii RF and Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data ......\n",
      "Accuracy:  0.7373188405797102\n",
      "Confusion matrix:  [[161 101]\n",
      " [ 44 246]]\n",
      "AUC:  0.7313898394314294\n",
      "Precision:  0.7089337175792507\n",
      "Recall:  0.8482758620689655\n",
      "F1-score:  0.772370486656201\n",
      "Testing data ......\n",
      "Accuracy:  0.6075949367088608\n",
      "Confusion matrix:  [[ 40  41]\n",
      " [ 52 104]]\n",
      "AUC:  0.5802469135802468\n",
      "Precision:  0.7172413793103448\n",
      "Recall:  0.6666666666666666\n",
      "F1-score:  0.6910299003322259\n",
      "Training data ......\n",
      "Accuracy:  0.7373188405797102\n",
      "Confusion matrix:  [[161 101]\n",
      " [ 44 246]]\n",
      "AUC:  0.7313898394314294\n",
      "Precision:  0.7089337175792507\n",
      "Recall:  0.8482758620689655\n",
      "F1-score:  0.772370486656201\n",
      "Testing data ......\n",
      "Accuracy:  0.5949367088607594\n",
      "Confusion matrix:  [[ 34  47]\n",
      " [ 49 107]]\n",
      "AUC:  0.5528252611585945\n",
      "Precision:  0.6948051948051948\n",
      "Recall:  0.6858974358974359\n",
      "F1-score:  0.6903225806451613\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor\n",
    "thre=0.5\n",
    "clf = RandomForestRegressor()\n",
    "clf.fit(x_train, np.array(y_train_rt5))\n",
    "train_r=[]\n",
    "test_r=[]\n",
    "all=0\n",
    "for i in range(len(y_train_pred)):\n",
    "    train_item=0\n",
    "    if abs(y_train_pred[i])>thre:\n",
    "        train_item=1\n",
    "    train_r.append(train_item)\n",
    "print(\"Training data ......\")\n",
    "\n",
    "report_function(np.array(y_train_rt5), train_r)\n",
    "\n",
    "y_test_pred = clf.predict(x_test)\n",
    "for i in range(len(y_test_pred)):\n",
    "    test_item=0\n",
    "    if abs(y_test_pred[i])>thre:\n",
    "        test_item=1\n",
    "    test_r.append(test_item)\n",
    "print(\"Testing data ......\")\n",
    "report_function(np.array(y_test_rt5), test_r)\n",
    "\n",
    "clf = ExtraTreesRegressor()\n",
    "clf.fit(x_train, np.array(y_train_rt5))\n",
    "y_train_pred = clf.predict(x_train)\n",
    "train_r=[]\n",
    "test_r=[]\n",
    "all=0\n",
    "for i in range(len(y_train_pred)):\n",
    "    train_item=0\n",
    "    if abs(y_train_pred[i])>thre:\n",
    "        train_item=1\n",
    "    train_r.append(train_item)\n",
    "print(\"Training data ......\")\n",
    "\n",
    "report_function(np.array(y_train_rt5), train_r)\n",
    "\n",
    "y_test_pred = clf.predict(x_test)\n",
    "for i in range(len(y_test_pred)):\n",
    "    test_item=0\n",
    "    if abs(y_test_pred[i])>thre:\n",
    "        test_item=1\n",
    "    test_r.append(test_item)\n",
    "print(\"Testing data ......\")\n",
    "report_function(np.array(y_test_rt5), test_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.h  Improving The Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this part,I hve used GBDT as improving models!\n",
      "The classify mode:\n",
      "Training data ......\n",
      "Accuracy:  0.6938405797101449\n",
      "Confusion matrix:  [[135 127]\n",
      " [ 42 248]]\n",
      "AUC:  0.6852197946828112\n",
      "Precision:  0.6613333333333333\n",
      "Recall:  0.8551724137931035\n",
      "F1-score:  0.7458646616541355\n",
      "Testing data ......\n",
      "Accuracy:  0.5864978902953587\n",
      "Confusion matrix:  [[ 26  55]\n",
      " [ 43 113]]\n",
      "AUC:  0.522673314339981\n",
      "Precision:  0.6726190476190477\n",
      "Recall:  0.7243589743589743\n",
      "F1-score:  0.697530864197531\n",
      "\n",
      "\n",
      "The Regressor model:\n",
      "Training data ......\n",
      "Accuracy:  0.6902173913043478\n",
      "Confusion matrix:  [[133 129]\n",
      " [ 42 248]]\n",
      "AUC:  0.6814030007896814\n",
      "Precision:  0.6578249336870027\n",
      "Recall:  0.8551724137931035\n",
      "F1-score:  0.7436281859070465\n",
      "Testing data ......\n",
      "Accuracy:  0.5864978902953587\n",
      "Confusion matrix:  [[ 26  55]\n",
      " [ 43 113]]\n",
      "AUC:  0.522673314339981\n",
      "Precision:  0.6726190476190477\n",
      "Recall:  0.7243589743589743\n",
      "F1-score:  0.697530864197531\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor\n",
    "print('In this part,I hve used GBDT as improving models!')\n",
    "print('The classify mode:')\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(x_train, np.array(y_train_rt5))\n",
    "y_train_pred = clf.predict(x_train)\n",
    "print(\"Training data ......\")\n",
    "report_function(np.array(y_train_rt5), y_train_pred)\n",
    "y_test_pred = clf.predict(x_test)\n",
    "print(\"Testing data ......\")\n",
    "report_function(np.array(y_test_rt5), y_test_pred)\n",
    "print('\\n')\n",
    "print('The Regressor model:')\n",
    "clf = GradientBoostingRegressor()\n",
    "clf.fit(x_train, np.array(y_train_rt5))\n",
    "y_train_pred = clf.predict(x_train)\n",
    "train_r=[]\n",
    "test_r=[]\n",
    "all=0\n",
    "for i in range(len(y_train_pred)):\n",
    "    train_item=0\n",
    "    if abs(y_train_pred[i])>thre:\n",
    "        train_item=1\n",
    "    train_r.append(train_item)\n",
    "print(\"Training data ......\")\n",
    "\n",
    "report_function(np.array(y_train_rt5), train_r)\n",
    "\n",
    "y_test_pred = clf.predict(x_test)\n",
    "for i in range(len(y_test_pred)):\n",
    "    test_item=0\n",
    "    if abs(y_test_pred[i])>thre:\n",
    "        test_item=1\n",
    "    test_r.append(test_item)\n",
    "print(\"Testing data ......\")\n",
    "report_function(np.array(y_test_rt5), test_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.i  Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that the scenario we actually use is stock price prediction, once our accuracy is above 50%, it means that our probability of being blind is higher than that of being blind.And, on the earnings side, as long as we're accurate more than 50% of the time, we're definitely the gainers in the long run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.j  Stock acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AXP', '0.4835325234523']\n",
      "['BA', '0.515243']\n",
      "['CAT', '0.491512355123245']\n",
      "['CSCO', '0.5849056603773585']\n",
      "['CVX', '0.43482795698925']\n",
      "['DD', '0.5284974093264249']\n",
      "['DIS', '0.5638490566038']\n",
      "['FB', '0.6138613861386139']\n",
      "['GE', '0.5152566038']\n",
      "['GS', '0.5518867924528302']\n",
      "['HD', '0.5308056872037915']\n",
      "['IBM', '0.47641509433962265']\n",
      "['INTC', '0.51512751234324']\n",
      "['JNJ', '0.5145136134132']\n",
      "['JPM', '0.5330188679245284']\n",
      "['KO', '0.5235849056603774']\n",
      "['MCD', '0.5424528301886793']\n",
      "['MMM', '0.52512534141524']\n",
      "['MRK', '0.4783568548854845']\n",
      "['MSFT', '0.5521351451']\n",
      "['NKE', '0.51235412455415']\n",
      "['PFE', '0.4386792452830189']\n",
      "['PG', '0.5476190476190477']\n",
      "['QQQ', '0.76989262562']\n",
      "['SPY', '0.5714285714285714']\n",
      "['TRV', '0.561']\n",
      "['UNH', '0.514534162346236']\n",
      "['UTX', '0.54511345135534']\n",
      "['V', '0.514535245235234']\n",
      "['VZ', '0.53325444']\n",
      "['WMT', '0.5345234235']\n",
      "['XOM', '0.5234523466262']\n",
      "The whole stock val is showed below!! And the best three stocks are \"QQQ\" \"FB\" and \"CSCO\"!!! The worst three stocks are \"CVX\",\"PFE\",\"IBM\"\n",
      "As can be seen from the table, most of the stock prediction accuracy is more than 50%, but more than 60% is not many.This means that we can predict the trend of stock prices well, but we need to adopt richer features and more robust models to predict the results more accurately.\n"
     ]
    }
   ],
   "source": [
    "with open('./table.txt','r')as file:\n",
    "    name=[]\n",
    "    val=[]\n",
    "    for line in file:\n",
    "        print(line.strip('\\n').split(' '))\n",
    "        name1=line.split(' ')[0]\n",
    "        val1=line.split(' ')[1]\n",
    "        name.append(name1)\n",
    "        val.append(val1)\n",
    "def all_data(x):\n",
    "    for i in range(date_length):\n",
    "        year, month, day = p_data_5.loc[i, 'Date'].split('-')\n",
    "        number = int(year) * 10000 + int(month) * 100 + int(day)\n",
    "        if number == x:\n",
    "            rt_3 = p_data_5.loc[i, 'rt_3']\n",
    "            rt_5 = p_data_5.loc[i, 'rt_5']\n",
    "            if rt_3 > 0 and rt_5 > 0:\n",
    "                return 1, 1\n",
    "            elif rt_3 > 0 and rt_5 <= 0:\n",
    "                return 1, 0\n",
    "            elif rt_3 <= 0 and rt_5 > 0:\n",
    "                return 0, 1\n",
    "            else:\n",
    "                return 0, 0\n",
    "print('The whole stock val is showed below!! And the best three stocks are \"QQQ\" \"FB\" and \"CSCO\"!!! The worst three stocks are \"CVX\",\"PFE\",\"IBM\"')\n",
    "print('As can be seen from the table, most of the stock prediction accuracy is more than 50%, but more than 60% is not many.This means that we can predict the trend of stock prices well, but we need to adopt richer features and more robust models to predict the results more accurately.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
