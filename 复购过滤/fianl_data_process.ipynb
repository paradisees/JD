{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_Glove_data done\n",
      "相关数据表读取完毕\n"
     ]
    }
   ],
   "source": [
    "#针对人工review完的数据，处理未被分类的单类商品和人工摘出来的错误商品和没有sku的商品\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def load_Glove_data():\n",
    "    item={}\n",
    "    with open('./Glove/vec_glv_475_rmcid3.txt', encoding='utf8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                vec=[]\n",
    "                pws=[pw_idx for pw_idx in line.strip().split(\" \")]\n",
    "                id=pws[0]\n",
    "                for content in pws[1:]:\n",
    "                    vec.append(float(content))\n",
    "                item[id]=np.array(vec).T\n",
    "            except:\n",
    "                continue\n",
    "    return item\n",
    "def _func(vec1,vec2):\n",
    "    #vec1 = vec_Glove[i]\n",
    "    #vec2 = vec_Glove[j]\n",
    "    val1 = np.sqrt(sum([item ** 2 for item in vec1]))\n",
    "    val2 = np.sqrt(sum([item ** 2 for item in vec2]))\n",
    "    sim = vec1.dot(vec2.T) / (val1 * val2)\n",
    "    return sim\n",
    "def load_data_relevant():\n",
    "    edges={}\n",
    "    with open('base_relevant_-1rmcid3_all', encoding='utf8') as file:\n",
    "        for line in file:\n",
    "            item=line.split(\":\")\n",
    "            key=item[0]\n",
    "            cons = [pw for pw in item[1].strip().split(\",\") if pw!='']\n",
    "            edges[key]=cons\n",
    "    return edges\n",
    "def rn_num():\n",
    "    rn_num={}\n",
    "    code_pw={}\n",
    "    for line in open('./id1_pw_num', encoding='utf8'):\n",
    "        cons = line.strip().split(\"\\t\")\n",
    "        try:\n",
    "            rn_num[cons[0]]=cons[2]\n",
    "            code_pw[cons[2]]=cons[0]\n",
    "        except:\n",
    "            print(cons[0])\n",
    "    return rn_num,code_pw\n",
    "def name_id(name):\n",
    "    return rn_num[name]\n",
    "\n",
    "vec_Glove=load_Glove_data()\n",
    "print(\"load_Glove_data done\")\n",
    "#rn序号对应的cid1-pw\n",
    "rn_num,code_pw=rn_num()\n",
    "edges=load_data_relevant()\n",
    "print('相关数据表读取完毕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of remove pw without sku is: 389\n",
      "0\n",
      "Data_merge is done!!\n",
      "无sku数据处理完毕，根据0.4阈值，总的需要处理个数为256，剩余未处理pw个数为256，已处理个数为195\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "Data_merge is done!!\n",
      "换用0.25阈值的数据处理完毕，总的需要处理个数为23731，剩余未处理pw个数为8992，已处理个数为14739\n"
     ]
    }
   ],
   "source": [
    "#----------------------人工review前的数据------------------------------------\n",
    "#读取根据阈值0.3处理的过的单类商品，如果在人工review阶段被‘-’掉，则不能再加入该大类\n",
    "'''def Data_processed_by_thre():\n",
    "    data_processed_by_thre=[]\n",
    "    with open('manual_judge_word_done_0.3', encoding='utf8') as file:\n",
    "        for line in file:\n",
    "            cons = [pw for pw in line.strip().split(\",\") if pw!='']\n",
    "            data_processed_by_thre.append(cons)\n",
    "    return data_processed_by_thre'''\n",
    "\n",
    "#读取未进入人工review阶段的单类商品,对他们进行阈值0.25调整后划分，剩下的单独成类\n",
    "def Data_without_processed_by_thre():\n",
    "    data_without_processed_by_thre=[]\n",
    "    with open('manual_judge_word_rm_0.3', encoding='utf8') as file:\n",
    "        for line in file:\n",
    "            cons = line.strip()\n",
    "            data_without_processed_by_thre.append(cons)\n",
    "    return data_without_processed_by_thre\n",
    "\n",
    "#----------------------人工review后的数据------------------------------------\n",
    "#读取人工review完毕后的总数据\n",
    "def load_review_data():\n",
    "    #分为分好类的数据，删选出来不属于该类的商品和没有sku的商品\n",
    "    data_done=[]\n",
    "    data_del=[]\n",
    "    data_del_cluster={}\n",
    "    data_without_sku=[]\n",
    "    with open('data_review', encoding='utf8') as file:\n",
    "        for line in file:\n",
    "            cons = [pw.strip() for pw in line.strip().split(\",\") if pw!='']\n",
    "            #print(cons[0][0])\n",
    "            if cons[0][0]=='-':\n",
    "                for item in cons:\n",
    "                    data_del.append(item.strip('-').strip())\n",
    "            elif cons[0][0]=='=':\n",
    "                for item in cons:\n",
    "                    data_without_sku.append(item.strip('=').strip())\n",
    "            else:\n",
    "                data_done.append(cons)\n",
    "    #print(data_done,data_del,data_without_sku)\n",
    "    return data_done,data_del,data_without_sku\n",
    "              \n",
    "              \n",
    "#已经被按0.3处理过的词[]\n",
    "#data_processed_by_thre=Data_processed_by_thre()\n",
    "#按照0.3仍未被处理的词，改用0.25阈值\n",
    "data_without_processed_by_thre=Data_without_processed_by_thre()\n",
    "#人工review完后，大类数据、删除数据和无sku数据\n",
    "data_done,data_del,data_without_sku=load_review_data()\n",
    "\n",
    "#----------------------首先针对无sku且排序>260000------------------------------------ \n",
    "#这部分处理完后，按照阈值0.4放入，否则认为找不到对应sku，单独成类\n",
    "def rm_pw_without_sku(data_without_sku):\n",
    "    data_without_sku_new=[]\n",
    "    rm_sku_num=0\n",
    "    for pw in data_without_sku:\n",
    "        if pw=='':\n",
    "            continue\n",
    "        if int(rn_num[pw])>=260000:\n",
    "            rm_sku_num+=1\n",
    "            continue\n",
    "        else:\n",
    "            data_without_sku_new.append(pw)\n",
    "    print('The number of remove pw without sku is:',rm_sku_num)\n",
    "    return data_without_sku_new\n",
    "data_without_sku=rm_pw_without_sku(data_without_sku)\n",
    "\n",
    "\n",
    "#----------------------定义合并数据的方法------------------------------------ \n",
    "#需要被归入的大类数据，需要合并的单类数据，输入阈值\n",
    "def data_merge(base,process,thre):\n",
    "    #维护长度为3的堆\n",
    "    top_n = 3\n",
    "    base_vec={}\n",
    "    base_id_code={}\n",
    "    base_id=0\n",
    "    #编码所在的list序列号\n",
    "    code_list={}\n",
    "    for cons in base:\n",
    "        vec=np.array([0.0 for i in range(300)])\n",
    "        tmp_code=[]\n",
    "        for item in cons:\n",
    "            if item=='' or item=='=' or item=='-':\n",
    "                continue\n",
    "            vec+=vec_Glove[name_id(item)]\n",
    "            tmp_code.append(name_id(item))\n",
    "            code_list[name_id(item)]=base_id\n",
    "        base_id_code[base_id]=tmp_code\n",
    "        base_vec[base_id]=vec/len(cons)\n",
    "        base_id+=1\n",
    "    remove_pw=[]\n",
    "    line=0\n",
    "    for pw in process:\n",
    "        if line%1000==0:\n",
    "            print(line)\n",
    "        line+=1\n",
    "        #vec_tmp为要处理的pw的词向量\n",
    "        vec_tmp = np.array(vec_Glove[name_id(pw)])\n",
    "        keys = []\n",
    "        vals = []\n",
    "        if name_id(pw) in edges.keys():\n",
    "            for edge in edges[name_id(pw)]:\n",
    "                try:\n",
    "                    vec_edge=vec_Glove[edge]\n",
    "                    val=_func(vec_tmp,vec_edge)\n",
    "                    if len(keys) < top_n:\n",
    "                            keys.append(edge)\n",
    "                            vals.append(val)\n",
    "                    else:\n",
    "                        min_val = vals[0]\n",
    "                        min_idx = 0\n",
    "                        for i in range(top_n):\n",
    "                            if vals[i] < min_val:\n",
    "                                min_val = vals[i]\n",
    "                                min_idx = i\n",
    "                        if val > min_val:\n",
    "                            keys[min_idx] = edge\n",
    "                            vals[min_idx] = val\n",
    "                except:\n",
    "                    continue\n",
    "        key_val={}\n",
    "        for i in range(len(keys)):\n",
    "            key_val[keys[i]]=vals[i]\n",
    "        new=sorted(key_val.items(),key=lambda x:x[1],reverse=True)\n",
    "        keys,vals=[],[]\n",
    "        for content in new:\n",
    "            keys.append(content[0])\n",
    "            vals.append(content[1])\n",
    "        for i in range(top_n):\n",
    "            try:\n",
    "                if keys[i] in code_list.keys():\n",
    "                    if vals[i]>thre:\n",
    "                        base[code_list[keys[i]]].append(pw)\n",
    "                        #满足单个在其他类中，即被处理的词\n",
    "                        if pw not in remove_pw:\n",
    "                            remove_pw.append(pw)\n",
    "                        break\n",
    "            except:                    \n",
    "                continue\n",
    "    process_left=[]\n",
    "    for item in process:\n",
    "        if item not in remove_pw:\n",
    "            process_left.append(item)\n",
    "    print('Data_merge is done!!')\n",
    "    return base,process_left,remove_pw\n",
    "\n",
    "\n",
    "#----------------------处理无sku且排序小于26W的词------------------------------------ \n",
    "data_done,data_without_sku_left,data_without_sku_remove_pw=data_merge(data_done,data_without_sku,0.4)\n",
    "print('无sku数据处理完毕，根据0.4阈值，总的需要处理个数为{}，剩余未处理pw个数为{}，已处理个数为{}'.format(len(data_without_sku_left),len(data_without_sku_left),len(data_without_sku_remove_pw)))\n",
    "\n",
    "#----------------------处理按照0.3仍未被处理的词，改用0.25阈值------------------------------------ \n",
    "data_done,data_without_processed_by_thre_left,data_without_processed_by_thre_remove_pw=data_merge(data_done,data_without_processed_by_thre,0.25)\n",
    "print('换用0.25阈值的数据处理完毕，总的需要处理个数为{}，剩余未处理pw个数为{}，已处理个数为{}'.format(len(data_without_processed_by_thre),len(data_without_processed_by_thre_left),len(data_without_processed_by_thre_remove_pw)))\n",
    "\n",
    "#----------------------处理人工review出来不符合类别的词------------------------------------ \n",
    "#针对剩余的单类商品，做二轮聚类处理\n",
    "#data_done,data_del_left,data_del_remove_pw=data_merge(data_done,data_del,0.25)\n",
    "\n",
    "\n",
    "\n",
    "#----------------------写入------------------------------------ \n",
    "#data_done+++data_without_sku_left+++++++data_without_processed_by_thre_left+++++\n",
    "\n",
    "file = open('data_review_out', 'w', encoding='utf8')\n",
    "for i in range(len(data_done)):\n",
    "    for j in range(len(data_done[i])):\n",
    "        file.write(data_done[i][j])\n",
    "        file.write(\", \" if j <len(data_done[i])-1 else \"\")\n",
    "    file.write(\"\\n\")\n",
    "file.write(\"-----------------------------------无sku的没有划分进大类的--------------------------------------------------------------------\")\n",
    "file.write(\"\\n\")\n",
    "for i in range(len(data_without_sku_left)):\n",
    "    file.write(data_without_sku_left[i])\n",
    "    file.write(\"\\n\")\n",
    "file.write(\"------------------------------------阈值小于0.25的单类-------------------------------------------------------------------\")\n",
    "file.write(\"\\n\")\n",
    "for i in range(len(data_without_processed_by_thre_left)):\n",
    "    file.write(data_without_processed_by_thre_left[i])\n",
    "    file.write(\"\\n\")\n",
    "file.write(\"-------------------------------------人工删除的单类------------------------------------------------------------------\")\n",
    "file.write(\"\\n\")\n",
    "for i in range(len(data_del)):\n",
    "    file.write(data_del[i])\n",
    "    file.write(\"\\n\")\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11510\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "相似矩阵计算完毕\n",
      "--------------------------------------------------\n",
      "MCL Parameters\n",
      "Expansion: 10\n",
      "Inflation: 1.1\n",
      "Pruning threshold: 0.005, frequency: 0.75 iteration\n",
      "Convergence check: 1 iteration\n",
      "Maximum iterations: 1000\n",
      "Sparse matrix mode\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Checking for convergence\n",
      "Iteration 2\n",
      "Checking for convergence\n",
      "Iteration 3\n",
      "Checking for convergence\n",
      "Iteration 4\n",
      "Checking for convergence\n",
      "Iteration 5\n",
      "Checking for convergence\n",
      "Iteration 6\n",
      "Checking for convergence\n",
      "Iteration 7\n",
      "Checking for convergence\n",
      "Iteration 8\n",
      "Checking for convergence\n",
      "Iteration 9\n",
      "Checking for convergence\n",
      "Iteration 10\n",
      "Checking for convergence\n",
      "Iteration 11\n",
      "Checking for convergence\n",
      "Iteration 12\n",
      "Checking for convergence\n",
      "Iteration 13\n",
      "Checking for convergence\n",
      "Iteration 14\n",
      "Checking for convergence\n",
      "Iteration 15\n",
      "Checking for convergence\n",
      "Iteration 16\n",
      "Checking for convergence\n",
      "Iteration 17\n",
      "Checking for convergence\n",
      "Iteration 18\n",
      "Checking for convergence\n",
      "Iteration 19\n",
      "Checking for convergence\n",
      "Iteration 20\n",
      "Checking for convergence\n",
      "Iteration 21\n",
      "Checking for convergence\n",
      "Iteration 22\n",
      "Checking for convergence\n",
      "Iteration 23\n",
      "Checking for convergence\n",
      "Iteration 24\n",
      "Checking for convergence\n",
      "Iteration 25\n",
      "Checking for convergence\n",
      "Iteration 26\n",
      "Checking for convergence\n",
      "Iteration 27\n",
      "Checking for convergence\n",
      "Iteration 28\n",
      "Checking for convergence\n",
      "Iteration 29\n",
      "Checking for convergence\n",
      "Iteration 30\n",
      "Checking for convergence\n",
      "Iteration 31\n",
      "Checking for convergence\n",
      "Iteration 32\n",
      "Checking for convergence\n",
      "Iteration 33\n",
      "Checking for convergence\n",
      "Iteration 34\n",
      "Checking for convergence\n",
      "Iteration 35\n",
      "Checking for convergence\n",
      "Iteration 36\n",
      "Checking for convergence\n",
      "Iteration 37\n",
      "Checking for convergence\n",
      "Iteration 38\n",
      "Checking for convergence\n",
      "Iteration 39\n",
      "Checking for convergence\n",
      "Iteration 40\n",
      "Checking for convergence\n",
      "Iteration 41\n",
      "Checking for convergence\n",
      "Iteration 42\n",
      "Checking for convergence\n",
      "Iteration 43\n",
      "Checking for convergence\n",
      "Iteration 44\n",
      "Checking for convergence\n",
      "Iteration 45\n",
      "Checking for convergence\n",
      "Iteration 46\n",
      "Checking for convergence\n",
      "Iteration 47\n",
      "Checking for convergence\n",
      "Iteration 48\n",
      "Checking for convergence\n",
      "Iteration 49\n",
      "Checking for convergence\n",
      "Iteration 50\n",
      "Checking for convergence\n",
      "Iteration 51\n",
      "Checking for convergence\n",
      "Iteration 52\n",
      "Checking for convergence\n",
      "Iteration 53\n",
      "Checking for convergence\n",
      "Iteration 54\n",
      "Checking for convergence\n",
      "Iteration 55\n",
      "Checking for convergence\n",
      "Iteration 56\n",
      "Checking for convergence\n",
      "Iteration 57\n",
      "Checking for convergence\n",
      "Iteration 58\n",
      "Checking for convergence\n",
      "Iteration 59\n",
      "Checking for convergence\n",
      "Iteration 60\n",
      "Checking for convergence\n",
      "Iteration 61\n",
      "Checking for convergence\n",
      "Iteration 62\n",
      "Checking for convergence\n",
      "Iteration 63\n",
      "Checking for convergence\n",
      "Iteration 64\n",
      "Checking for convergence\n",
      "Iteration 65\n",
      "Checking for convergence\n",
      "Iteration 66\n",
      "Checking for convergence\n",
      "Iteration 67\n",
      "Checking for convergence\n",
      "Iteration 68\n",
      "Checking for convergence\n",
      "Iteration 69\n",
      "Checking for convergence\n",
      "Iteration 70\n",
      "Checking for convergence\n",
      "Iteration 71\n",
      "Checking for convergence\n",
      "Iteration 72\n",
      "Checking for convergence\n",
      "Iteration 73\n",
      "Checking for convergence\n",
      "Iteration 74\n",
      "Checking for convergence\n",
      "Iteration 75\n",
      "Checking for convergence\n",
      "Iteration 76\n",
      "Checking for convergence\n",
      "Iteration 77\n",
      "Checking for convergence\n",
      "Iteration 78\n",
      "Checking for convergence\n",
      "Iteration 79\n",
      "Checking for convergence\n",
      "Iteration 80\n",
      "Checking for convergence\n",
      "Iteration 81\n",
      "Checking for convergence\n",
      "Iteration 82\n",
      "Checking for convergence\n",
      "Iteration 83\n",
      "Checking for convergence\n",
      "Iteration 84\n",
      "Checking for convergence\n",
      "Iteration 85\n",
      "Checking for convergence\n",
      "Iteration 86\n",
      "Checking for convergence\n",
      "Iteration 87\n",
      "Checking for convergence\n",
      "Iteration 88\n",
      "Checking for convergence\n",
      "Iteration 89\n",
      "Checking for convergence\n",
      "Iteration 90\n",
      "Checking for convergence\n",
      "Iteration 91\n",
      "Checking for convergence\n",
      "Iteration 92\n",
      "Checking for convergence\n",
      "Iteration 93\n",
      "Checking for convergence\n",
      "Iteration 94\n",
      "Checking for convergence\n",
      "Iteration 95\n",
      "Checking for convergence\n",
      "Converged after 95 iterations\n",
      "--------------------------------------------------\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "#处理类中单个商品间的相似度\n",
    "import markov_clustering as mc\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from  gensim import models\n",
    "from scipy.sparse import *\n",
    "from collections import defaultdict\n",
    "def load_multy_data():\n",
    "    data={}\n",
    "    data_vec={}\n",
    "    num_id=0\n",
    "    #记录num_id对应的edges的编号，方便后面找到相连的边;以及edges对应的num_id\n",
    "    num_id_edges={}\n",
    "    with open('one_cluster_pw', encoding='utf8') as file:     \n",
    "        for line in file:\n",
    "            cons = line.strip()\n",
    "            vec=vec_Glove[str(rn_num[cons])]\n",
    "            data[str(num_id)]=cons\n",
    "            data_vec[str(num_id)]=vec\n",
    "            num_id_edges[num_id]=str(rn_num[cons])\n",
    "            num_id+=1\n",
    "    return data,data_vec,num_id,num_id_edges\n",
    "data,data_vec,num_id,num_id_edges=load_multy_data()\n",
    "\n",
    "expansion = 10  # 使得不同的区域之间的联系加强\n",
    "inflation = 1.1  # 各个数据幂次方，强化紧密的点，弱化松散的点\n",
    "loop_value = 1  # 自循环的概率\n",
    "iterations = 1000\n",
    "pruning_threshold = 0.005\n",
    "pruning_frequency = 0.75              \n",
    "class MCL(object):\n",
    "    def __init__(self, id2pw: dict):\n",
    "        self.id2pw = id2pw\n",
    "        self.cluster_name = \"one_review_cluster\"\n",
    "        self.rbcids = []\n",
    "\n",
    "    def run_from_mat(self, sim_mat):\n",
    "        result = mc.run_mcl(sim_mat,\n",
    "                            expansion=expansion,\n",
    "                            inflation=inflation,\n",
    "                            loop_value=loop_value,\n",
    "                            iterations=iterations,\n",
    "                            pruning_threshold=pruning_threshold,\n",
    "                            pruning_frequency=pruning_frequency,\n",
    "                            verbose=True\n",
    "                            )  # run MCL with default parameters\n",
    "        rbcids = mc.get_clusters(result)  # get clusters\n",
    "        \n",
    "        for item in rbcids:\n",
    "            sub_item = []\n",
    "            for i in range(len(item)):\n",
    "                pw = self.id2pw.get(str(item[i]))\n",
    "                if pw is not None:\n",
    "                    sub_item.append(pw)\n",
    "            self.rbcids.append(sub_item)\n",
    "        #print(self.rbcids)\n",
    "\n",
    "    def save_cluster(self):\n",
    "        file = open(self.cluster_name, 'w', encoding='utf8')\n",
    "        file.write(\"#############\\n\")\n",
    "        file.write(\"expansion={0}\\ninflation={1}\\n\"\n",
    "                   \"loop_value={2}\\niterations={3}\\n\"\n",
    "                   \"pruning_threshold={4}\\n\"\n",
    "                   \"pruning_frequency={5}\\n\".format(expansion, inflation,\n",
    "                                                    loop_value, iterations,\n",
    "                                                    pruning_threshold,\n",
    "                                                    pruning_frequency))\n",
    "        file.write(\"#############\\n\")\n",
    "        for item in self.rbcids:\n",
    "            for i in range(len(item)):\n",
    "                pw = item[i]\n",
    "                try:\n",
    "                    #cid1=pw.split('-')[0]\n",
    "                    #cid1_name=id1_name[cid1]\n",
    "                    #pw=cid1_name+'-'+pw.split('-')[1]\n",
    "                    file.write(pw+\", \" if i < len(item)-1 else pw)\n",
    "                except:\n",
    "                    print(pw)\n",
    "            file.write(\"\\n\")\n",
    "        file.close()\n",
    "def graph_process():\n",
    "    print(num_id)\n",
    "    sim_mat = lil_matrix((num_id, num_id))\n",
    "    for i in range(num_id-2):\n",
    "        if i%1000==0:\n",
    "            print(i)\n",
    "        lst = edges.get(num_id_edges[i])\n",
    "        for j in range(i+1,num_id):\n",
    "            if num_id_edges[j] in lst:\n",
    "                val=_func(data_vec[str(i)],data_vec[str(j)])\n",
    "                try:\n",
    "                    if val>0.2:\n",
    "                        sim_mat[i, j] = val\n",
    "                        sim_mat[j, i] = val\n",
    "                except Exception as e:\n",
    "                    #print(e) \n",
    "                    continue\n",
    "    return sim_mat\n",
    "\n",
    "graph_dict=graph_process()\n",
    "print('相似矩阵计算完毕')\n",
    "A=MCL(data)\n",
    "A.run_from_mat(graph_dict)\n",
    "A.save_cluster()\n",
    "print('All done')              \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'服饰内衣-腹肌'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6fd283e36d50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mnum_id\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_vec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_id_edges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_vec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_id_edges\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_multy_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mexpansion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m  \u001b[0;31m# 使得不同的区域之间的联系加强\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-6fd283e36d50>\u001b[0m in \u001b[0;36mload_multy_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m''\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mvec\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_Glove\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrn_num\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0mnum_id_edges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrn_num\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mvec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '服饰内衣-腹肌'"
     ]
    }
   ],
   "source": [
    "#处理类中多个商品间的相似度\n",
    "import markov_clustering as mc\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from  gensim import models\n",
    "from scipy.sparse import *\n",
    "from collections import defaultdict\n",
    "def load_multy_data():\n",
    "    data={}\n",
    "    data_vec={}\n",
    "    num_id=0\n",
    "    #记录num_id对应的edges的编号，方便后面找到相连的边;以及edges对应的num_id\n",
    "    num_id_edges={}\n",
    "    with open('multy_test_review.txt', encoding='utf8') as file:     \n",
    "        for line in file:\n",
    "            flag=[item for item in line.split(',')]\n",
    "            if len(flag)==1:\n",
    "                continue\n",
    "            cons=[]\n",
    "            vec=np.array([0.0 for i in range(300)])\n",
    "            num_id_edges[num_id]=[]\n",
    "            for item in line.strip().split(','):\n",
    "                if item=='':\n",
    "                    continue\n",
    "                item=item.strip()\n",
    "                cons.append(item)\n",
    "                if item=='' or item=='=':\n",
    "                    continue\n",
    "                vec+=np.array(vec_Glove[str(rn_num[item])])\n",
    "                num_id_edges[num_id]+=edges.get(str(rn_num[item]))\n",
    "            vec=vec/len(cons)\n",
    "            data[str(num_id)]=cons\n",
    "            data_vec[str(num_id)]=vec\n",
    "            num_id+=1\n",
    "    return data,data_vec,num_id,num_id_edges\n",
    "data,data_vec,num_id,num_id_edges=load_multy_data()\n",
    "\n",
    "expansion = 2  # 使得不同的区域之间的联系加强\n",
    "inflation = 1.1  # 各个数据幂次方，强化紧密的点，弱化松散的点\n",
    "loop_value = 1  # 自循环的概率\n",
    "iterations = 1000\n",
    "pruning_threshold = 0.005\n",
    "pruning_frequency = 0.75              \n",
    "class MCL(object):\n",
    "    def __init__(self, id2pw: dict):\n",
    "        self.id2pw = id2pw\n",
    "        self.cluster_name = \"multy_test_cluster\"\n",
    "        self.rbcids = []\n",
    "\n",
    "    def run_from_mat(self, sim_mat):\n",
    "        result = mc.run_mcl(sim_mat,\n",
    "                            expansion=expansion,\n",
    "                            inflation=inflation,\n",
    "                            loop_value=loop_value,\n",
    "                            iterations=iterations,\n",
    "                            pruning_threshold=pruning_threshold,\n",
    "                            pruning_frequency=pruning_frequency,\n",
    "                            verbose=False\n",
    "                            )  # run MCL with default parameters\n",
    "        rbcids = mc.get_clusters(result)  # get clusters\n",
    "        \n",
    "        for item in rbcids:\n",
    "            print(item)\n",
    "            sub_item = []\n",
    "            for i in range(len(item)):\n",
    "                pw = self.id2pw.get(str(item[i]))\n",
    "                if pw is not None:\n",
    "                    sub_item.append(pw)\n",
    "            self.rbcids.append(sub_item)\n",
    "    def save_cluster(self):\n",
    "        file = open(self.cluster_name, 'w', encoding='utf8')\n",
    "        file.write(\"#############\\n\")\n",
    "        file.write(\"expansion={0}\\ninflation={1}\\n\"\n",
    "                   \"loop_value={2}\\niterations={3}\\n\"\n",
    "                   \"pruning_threshold={4}\\n\"\n",
    "                   \"pruning_frequency={5}\\n\".format(expansion, inflation,\n",
    "                                                    loop_value, iterations,\n",
    "                                                    pruning_threshold,\n",
    "                                                    pruning_frequency))\n",
    "        file.write(\"#############\\n\")\n",
    "        for item in self.rbcids:\n",
    "            t=[]\n",
    "            out=[t.extend(i) for i in item]\n",
    "            #print (out)\n",
    "            for i in range(len(item)):\n",
    "                pw = item[i]\n",
    "                #print(pw)\n",
    "                try:\n",
    "                    for i in range(len(pw)):\n",
    "                        file.write(pw[i]+',' if i<len(pw)-1 else pw[i])\n",
    "                except:\n",
    "                    print(item)\n",
    "            file.write('\\n')       \n",
    "        file.close()\n",
    "def graph_process():\n",
    "    print(num_id)\n",
    "    sim_mat = lil_matrix((num_id, num_id))\n",
    "    for i in range(num_id-2):\n",
    "        if i%1000==0:\n",
    "            print(i)\n",
    "        for j in range(i+1,num_id):\n",
    "                val=_func(data_vec[str(i)],data_vec[str(j)])\n",
    "                try:\n",
    "                    if val>0.5:\n",
    "                        sim_mat[i, j] = val\n",
    "                        sim_mat[j, i] = val\n",
    "                except Exception as e:\n",
    "                    #print(e) \n",
    "                    continue\n",
    "    return sim_mat\n",
    "graph_dict=graph_process()\n",
    "#print(graph_dict)\n",
    "print('相似矩阵计算完毕')\n",
    "A=MCL(data)\n",
    "A.run_from_mat(graph_dict)\n",
    "A.save_cluster()\n",
    "print('All done')              \n",
    "               \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Data_merge is done!!\n",
      "换用0.25阈值的数据处理完毕，总的需要处理个数为191，剩余未处理pw个数为11，已处理个数为180\n",
      "[['珠宝首饰-项链', '珠宝首饰-吊坠', '珠宝首饰-锁骨链', '珠宝首饰-黄金吊坠', '珠宝首饰-银项链', '珠宝首饰-挂坠', '珠宝首饰-颈链', '珠宝首饰-项坠', '珠宝首饰-金吊坠', '珠宝首饰-金项链', '珠宝首饰-纯银项链', '珠宝首饰-佛吊坠', '珠宝首饰-观音', '珠宝首饰-项圈', '珠宝首饰-护身符', '珠宝首饰-颈带', '珠宝首饰-银吊坠', '珠宝首饰-长项链', '珠宝首饰-金坠子', '珠宝首饰-足银项链', '珠宝首饰-颈圈', '珠宝首饰-短项链', '珠宝首饰-银项圈', '珠宝首饰-脖圈', '珠宝首饰-毛衣链', '珠宝首饰-挂链', '珠宝首饰-项链饰品', '珠宝首饰-脖链', '珠宝首饰-单链子', '二手商品-长项链', '二手商品-银项链', '二手商品-挂链', '二手商品-黄金吊坠', '二手商品-金吊坠', '二手商品-颈链', '珠宝首饰-首饰饰品', '珠宝首饰-脖颈链', '珠宝首饰-算盘', '珠宝首饰-项链坠子', '珠宝首饰-小吊坠', '珠宝首饰-链坠', '珠宝首饰-麋鹿角', '珠宝首饰-银牌', '珠宝首饰-骨链', '珠宝首饰-衣链', '珠宝首饰-吊牌', '珠宝首饰-大吊坠', '珠宝首饰-银坠子', '珠宝首饰-阴阳鱼', '珠宝首饰-佛坠', '珠宝首饰-金项链坠', '珠宝首饰-许愿石', '珠宝首饰-萌鸡', '珠宝首饰-蜜豆', '珠宝首饰-猫项链', '珠宝首饰-银坠', '珠宝首饰-转运石', '珠宝首饰-颈饰品', '珠宝首饰-权杖', '珠宝首饰-福字贴', '珠宝首饰-名字牌', '珠宝首饰-灵符', '珠宝首饰-手工皂', '珠宝首饰-玉树', '珠宝首饰-汉白玉', '珠宝首饰-楼梯', '珠宝首饰-鸡尾酒', '珠宝首饰-白骨精', '珠宝首饰-舞裙', '珠宝首饰-翡翠兔', '珠宝首饰-摆钟', '珠宝首饰-花籽', '珠宝首饰-钥匙环扣', '珠宝首饰-相机', '珠宝首饰-关公刀', '珠宝首饰-花板', '家居日用-护甲', '珠宝首饰-浮萍', '珠宝首饰-机油', '珠宝首饰-锤头', '珠宝首饰-腹轮', '珠宝首饰-奶昔杯', '珠宝首饰-拎包', '珠宝首饰-车轮胎', '珠宝首饰-爱心杯子', '珠宝首饰-鱼锁', '珠宝首饰-八爪鱼', '珠宝首饰-土陶', '珠宝首饰-三开盒', '珠宝首饰-梅瓶', '珠宝首饰-折叠刀', '珠宝首饰-手托', '珠宝首饰-镜子盒', '珠宝首饰-尾翼', '珠宝首饰-风叶', '珠宝首饰-小豌豆', '珠宝首饰-杜宾犬', '珠宝首饰-老虎锁', '珠宝首饰-美金', '珠宝首饰-镂空球', '珠宝首饰-猫吊坠', '珠宝首饰-彩石', '珠宝首饰-三生石', '珠宝首饰-龙猫项链', '珠宝首饰-铺梦网', '珠宝首饰-小吊牌', '珠宝首饰-松塔', '珠宝首饰-平衡木', '珠宝首饰-金链条', '珠宝首饰-打标机', '珠宝首饰-银钢', '珠宝首饰-黑蜡', '珠宝首饰-蚕豆', '珠宝首饰-柯基犬', '珠宝首饰-锄头', '珠宝首饰-花鼓', '珠宝首饰-小礼服', '珠宝首饰-铁锤', '珠宝首饰-水车', '珠宝首饰-龙息石', '珠宝首饰-马蹄铁', '珠宝首饰-礼物卡', '珠宝首饰-水晶板', '珠宝首饰-刀子', '珠宝首饰-云台', '珠宝首饰-蓝虎石'], ['珠宝首饰-手链', '珠宝首饰-手镯', '珠宝首饰-硬金', '珠宝首饰-转运珠', '珠宝首饰-貔貅', '珠宝首饰-配饰', '珠宝首饰-首饰', '珠宝首饰-手饰', '珠宝首饰-套链', '珠宝首饰-生肖猪', '珠宝首饰-银手链', '珠宝首饰-链子', '珠宝首饰-金珠', '珠宝首饰-礼物', '珠宝首饰-鞋带', '珠宝首饰-金饰', '珠宝首饰-银链', '珠宝首饰-银链子', '珠宝首饰-配链', '珠宝首饰-单链', '珠宝首饰-水波链', '珠宝首饰-o字链', '珠宝首饰-盒子链', '珠宝首饰-手链饰品', '珠宝首饰-瓜子链', '珠宝首饰-蛇骨链', '珠宝首饰-十字链', '珠宝首饰-延长链', '珠宝首饰-金钱币', '珠宝首饰-元宝链', '珠宝首饰-银饰品', '珠宝首饰-金饰品', '珠宝首饰-银手串', '珠宝首饰-手带', '二手商品-手带', '二手商品-金饰', '珠宝首饰-黑玛瑙', '珠宝首饰-猪手链', '珠宝首饰-珍珠吊坠', '珠宝首饰-陶瓷', '珠宝首饰-纳福', '珠宝首饰-节节高', '珠宝首饰-珠链', '珠宝首饰-捕梦网', '珠宝首饰-礼盒', '珠宝首饰-纯银貔貅', '珠宝首饰-瓜子', '珠宝首饰-玛瑙珠', '珠宝首饰-蝴蝶结', '珠宝首饰-手饰饰品', '珠宝首饰-学生鞋', '珠宝首饰-k金链', '珠宝首饰-珍珠链', '珠宝首饰-缅因猫', '珠宝首饰-黄金珠', '珠宝首饰-钢丝绳', '珠宝首饰-纯银链', '珠宝首饰-海棠', '珠宝首饰-米奇', '珠宝首饰-银杏叶', '珠宝首饰-种子', '珠宝首饰-加长链', '珠宝首饰-手腕饰品', '珠宝首饰-折纸', '珠宝首饰-蛇链', '珠宝首饰-车子', '珠宝首饰-字母链', '珠宝首饰-仿钻饰品', '珠宝首饰-配链子', '珠宝首饰-星盘', '珠宝首饰-银手饰', '珠宝首饰-红皮绳', '珠宝首饰-银黄', '珠宝首饰-镀银项链', '珠宝首饰-天使蛋', '珠宝首饰-对象', '珠宝首饰-尖刀', '珠宝首饰-枣木', '珠宝首饰-芍药花', '珠宝首饰-招财猪', '珠宝首饰-延长扣', '珠宝首饰-婚纱照', '珠宝首饰-黄金莲花', '珠宝首饰-椰子', '珠宝首饰-伸缩链', '珠宝首饰-菠萝花', '珠宝首饰-银镂空球', '珠宝首饰-转轮', '珠宝首饰-企鹅', '珠宝首饰-展台', '珠宝首饰-囚服', '珠宝首饰-盒子包', '珠宝首饰-情侣鞋', '珠宝首饰-窝料', '珠宝首饰-纸巾夹', '珠宝首饰-光油', '珠宝首饰-禅杖', '珠宝首饰-珐琅器', '珠宝首饰-白纱', '珠宝首饰-凝脂', '珠宝首饰-反光绳', '珠宝首饰-黑牡丹', '珠宝首饰-红豆', '珠宝首饰-麻花线', '珠宝首饰-双头龙', '珠宝首饰-粗麻绳', '珠宝首饰-奶嘴', '珠宝首饰-微雕', '珠宝首饰-囍字', '珠宝首饰-草绳', '珠宝首饰-黑胆石', '珠宝首饰-招财猫', '珠宝首饰-墨玉坠', '珠宝首饰-金六福', '珠宝首饰-铁链', '珠宝首饰-黄金花', '珠宝首饰-麦粒', '珠宝首饰-延长线', '珠宝首饰-图传', '珠宝首饰-野草', '珠宝首饰-花月夜', '珠宝首饰-五星花', '珠宝首饰-无毛猫', '珠宝首饰-黄板', '珠宝首饰-砂石']]\n",
      "----------------------\n",
      "['珠宝首饰-天鹅', '珠宝首饰-刀刀狗', '珠宝首饰-小挂坠', '珠宝首饰-锁骨', '珠宝首饰-汽球', '珠宝首饰-圆几', '珠宝首饰-马甲', '珠宝首饰-玫瑰花盒', '珠宝首饰-拨片盒', '珠宝首饰-包装箱', '珠宝首饰-卡纸']\n"
     ]
    }
   ],
   "source": [
    "#手動處理類別中的商品\n",
    "#----------------------人工review前的数据------------------------------------\n",
    "\n",
    "#读取未进入人工review阶段的单类商品,对他们进行阈值0.25调整后划分，剩下的单独成类\n",
    "def Data_without_processed_by_thre():\n",
    "    data_without_processed_by_thre=[]\n",
    "    with open('manual_judge_word_rm_test', encoding='utf8') as file:\n",
    "        for line in file:\n",
    "            cons = line.strip()\n",
    "            data_without_processed_by_thre.append(cons)\n",
    "    return data_without_processed_by_thre\n",
    "\n",
    "#----------------------人工review后的数据------------------------------------\n",
    "#读取人工review完毕后的总数据\n",
    "def load_review_data():\n",
    "    #分为分好类的数据，删选出来不属于该类的商品和没有sku的商品\n",
    "    data_done=[]\n",
    "    data_del=[]\n",
    "    data_del_cluster={}\n",
    "    data_without_sku=[]\n",
    "    with open('data_review_test', encoding='utf8') as file:\n",
    "        for line in file:\n",
    "            cons = [pw.strip() for pw in line.strip().split(\",\") if pw!='']\n",
    "            data_done.append(cons)\n",
    "    return data_done,data_del,data_without_sku\n",
    "              \n",
    "\n",
    "#按照0.3仍未被处理的词，改用0.25阈值\n",
    "data_without_processed_by_thre=Data_without_processed_by_thre()\n",
    "data_done,data_del,data_without_sku=load_review_data()\n",
    "\n",
    "\n",
    "#----------------------定义合并数据的方法------------------------------------ \n",
    "#需要被归入的大类数据，需要合并的单类数据，输入阈值\n",
    "def data_merge(base,process,thre):\n",
    "    #维护长度为3的堆\n",
    "    top_n = 3\n",
    "    base_vec={}\n",
    "    base_id_code={}\n",
    "    base_id=0\n",
    "    #编码所在的list序列号\n",
    "    code_list={}\n",
    "    for cons in base:\n",
    "        vec=np.array([0.0 for i in range(300)])\n",
    "        tmp_code=[]\n",
    "        for item in cons:\n",
    "            if item=='' or item=='=' or item=='-':\n",
    "                continue\n",
    "            vec+=vec_Glove[name_id(item)]\n",
    "            tmp_code.append(name_id(item))\n",
    "            code_list[name_id(item)]=base_id\n",
    "        base_id_code[base_id]=tmp_code\n",
    "        base_vec[base_id]=vec/len(cons)\n",
    "        base_id+=1\n",
    "    remove_pw=[]\n",
    "    line=0\n",
    "    for pw in process:\n",
    "        if line%1000==0:\n",
    "            print(line)\n",
    "        line+=1\n",
    "        #vec_tmp为要处理的pw的词向量\n",
    "        vec_tmp = np.array(vec_Glove[name_id(pw)])\n",
    "        keys = []\n",
    "        vals = []\n",
    "        if name_id(pw) in edges.keys():\n",
    "            for edge in edges[name_id(pw)]:\n",
    "                try:\n",
    "                    vec_edge=vec_Glove[edge]\n",
    "                    val=_func(vec_tmp,vec_edge)\n",
    "                    if len(keys) < top_n:\n",
    "                            keys.append(edge)\n",
    "                            vals.append(val)\n",
    "                    else:\n",
    "                        min_val = vals[0]\n",
    "                        min_idx = 0\n",
    "                        for i in range(top_n):\n",
    "                            if vals[i] < min_val:\n",
    "                                min_val = vals[i]\n",
    "                                min_idx = i\n",
    "                        if val > min_val:\n",
    "                            keys[min_idx] = edge\n",
    "                            vals[min_idx] = val\n",
    "                except:\n",
    "                    continue\n",
    "        key_val={}\n",
    "        for i in range(len(keys)):\n",
    "            key_val[keys[i]]=vals[i]\n",
    "        new=sorted(key_val.items(),key=lambda x:x[1],reverse=True)\n",
    "        keys,vals=[],[]\n",
    "        for content in new:\n",
    "            keys.append(content[0])\n",
    "            vals.append(content[1])\n",
    "        for i in range(top_n):\n",
    "            try:\n",
    "                if keys[i] in code_list.keys():\n",
    "                    if vals[i]>thre:\n",
    "                        base[code_list[keys[i]]].append(pw)\n",
    "                        #满足单个在其他类中，即被处理的词\n",
    "                        if pw not in remove_pw:\n",
    "                            remove_pw.append(pw)\n",
    "                        break\n",
    "            except:                    \n",
    "                continue\n",
    "    process_left=[]\n",
    "    for item in process:\n",
    "        if item not in remove_pw:\n",
    "            process_left.append(item)\n",
    "    print('Data_merge is done!!')\n",
    "    return base,process_left,remove_pw\n",
    "\n",
    "\n",
    "#----------------------处理按照0.3仍未被处理的词，改用0.25阈值------------------------------------ \n",
    "data_done,data_without_processed_by_thre_left,data_without_processed_by_thre_remove_pw=data_merge(data_done,data_without_processed_by_thre,0)\n",
    "print('换用0.25阈值的数据处理完毕，总的需要处理个数为{}，剩余未处理pw个数为{}，已处理个数为{}'.format(len(data_without_processed_by_thre),len(data_without_processed_by_thre_left),len(data_without_processed_by_thre_remove_pw)))\n",
    "\n",
    "print(data_done)\n",
    "print('----------------------')\n",
    "print(data_without_processed_by_thre_left)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
